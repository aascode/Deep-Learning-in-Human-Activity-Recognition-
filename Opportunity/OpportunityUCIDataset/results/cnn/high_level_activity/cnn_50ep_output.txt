Train on 20508 samples, validate on 13673 samples
Epoch 1/50
20508/20508 [==============================] - 78s 4ms/sample - loss: 0.7560 - acc: 0.7625 - val_loss: 0.3895 - val_acc: 0.8848
Epoch 2/50
20508/20508 [==============================] - 83s 4ms/sample - loss: 0.4152 - acc: 0.8744 - val_loss: 0.3458 - val_acc: 0.9029
Epoch 3/50
20508/20508 [==============================] - 88s 4ms/sample - loss: 0.3147 - acc: 0.9111 - val_loss: 0.1940 - val_acc: 0.9604
Epoch 4/50
20508/20508 [==============================] - 97s 5ms/sample - loss: 0.2651 - acc: 0.9324 - val_loss: 0.1747 - val_acc: 0.9677
Epoch 5/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.2384 - acc: 0.9451 - val_loss: 0.2025 - val_acc: 0.9641
Epoch 6/50
20508/20508 [==============================] - 97s 5ms/sample - loss: 0.2284 - acc: 0.9528 - val_loss: 0.1584 - val_acc: 0.9769
Epoch 7/50
20508/20508 [==============================] - 96s 5ms/sample - loss: 0.2093 - acc: 0.9604 - val_loss: 0.1858 - val_acc: 0.9714
Epoch 8/50
20508/20508 [==============================] - 96s 5ms/sample - loss: 0.1990 - acc: 0.9644 - val_loss: 0.1542 - val_acc: 0.9841
Epoch 9/50
20508/20508 [==============================] - 97s 5ms/sample - loss: 0.2018 - acc: 0.9664 - val_loss: 0.1562 - val_acc: 0.9838
Epoch 10/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1954 - acc: 0.9701 - val_loss: 0.1667 - val_acc: 0.9825
Epoch 11/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1935 - acc: 0.9711 - val_loss: 0.1574 - val_acc: 0.9872
Epoch 12/50
20508/20508 [==============================] - 99s 5ms/sample - loss: 0.1978 - acc: 0.9711 - val_loss: 0.1588 - val_acc: 0.9873
Epoch 13/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1875 - acc: 0.9766 - val_loss: 0.1529 - val_acc: 0.9876
Epoch 14/50
20508/20508 [==============================] - 103s 5ms/sample - loss: 0.1816 - acc: 0.9773 - val_loss: 0.1480 - val_acc: 0.9881
Epoch 15/50
20508/20508 [==============================] - 104s 5ms/sample - loss: 0.1858 - acc: 0.9744 - val_loss: 0.1634 - val_acc: 0.9858
Epoch 16/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1858 - acc: 0.9765 - val_loss: 0.1537 - val_acc: 0.9907
Epoch 17/50
20508/20508 [==============================] - 99s 5ms/sample - loss: 0.1923 - acc: 0.9764 - val_loss: 0.1495 - val_acc: 0.9914
Epoch 18/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1887 - acc: 0.9780 - val_loss: 0.1498 - val_acc: 0.9919
Epoch 19/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1851 - acc: 0.9789 - val_loss: 0.1630 - val_acc: 0.9876
Epoch 20/50
20508/20508 [==============================] - 97s 5ms/sample - loss: 0.1888 - acc: 0.9791 - val_loss: 0.1694 - val_acc: 0.9851
Epoch 21/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1774 - acc: 0.9802 - val_loss: 0.1574 - val_acc: 0.9881
Epoch 22/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1843 - acc: 0.9791 - val_loss: 0.1571 - val_acc: 0.9906
Epoch 23/50
20508/20508 [==============================] - 100s 5ms/sample - loss: 0.1694 - acc: 0.9823 - val_loss: 0.1526 - val_acc: 0.9917
Epoch 24/50
20508/20508 [==============================] - 104s 5ms/sample - loss: 0.1732 - acc: 0.9819 - val_loss: 0.1474 - val_acc: 0.9909
Epoch 25/50
20508/20508 [==============================] - 102s 5ms/sample - loss: 0.1769 - acc: 0.9814 - val_loss: 0.1554 - val_acc: 0.9894
Epoch 26/50
20508/20508 [==============================] - 103s 5ms/sample - loss: 0.1760 - acc: 0.9820 - val_loss: 0.1511 - val_acc: 0.9906
Epoch 27/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1719 - acc: 0.9830 - val_loss: 0.1525 - val_acc: 0.9890
Epoch 28/50
20508/20508 [==============================] - 102s 5ms/sample - loss: 0.1664 - acc: 0.9844 - val_loss: 0.1383 - val_acc: 0.9928
Epoch 29/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1844 - acc: 0.9785 - val_loss: 0.1547 - val_acc: 0.9922
Epoch 30/50
20508/20508 [==============================] - 100s 5ms/sample - loss: 0.1704 - acc: 0.9839 - val_loss: 0.1480 - val_acc: 0.9913
Epoch 31/50
20508/20508 [==============================] - 100s 5ms/sample - loss: 0.1735 - acc: 0.9822 - val_loss: 0.1456 - val_acc: 0.9931
Epoch 32/50
20508/20508 [==============================] - 100s 5ms/sample - loss: 0.1743 - acc: 0.9826 - val_loss: 0.1438 - val_acc: 0.9931
Epoch 33/50
20508/20508 [==============================] - 99s 5ms/sample - loss: 0.1654 - acc: 0.9852 - val_loss: 0.1516 - val_acc: 0.9919
Epoch 34/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1711 - acc: 0.9831 - val_loss: 0.1477 - val_acc: 0.9931
Epoch 35/50
20508/20508 [==============================] - 98s 5ms/sample - loss: 0.1681 - acc: 0.9857 - val_loss: 0.1409 - val_acc: 0.9947
Epoch 36/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1637 - acc: 0.9851 - val_loss: 0.1442 - val_acc: 0.9937
Epoch 37/50
20508/20508 [==============================] - 102s 5ms/sample - loss: 0.1708 - acc: 0.9843 - val_loss: 0.1519 - val_acc: 0.9917
Epoch 38/50
20508/20508 [==============================] - 103s 5ms/sample - loss: 0.1616 - acc: 0.9860 - val_loss: 0.1368 - val_acc: 0.9938
Epoch 39/50
20508/20508 [==============================] - 104s 5ms/sample - loss: 0.1632 - acc: 0.9855 - val_loss: 0.1336 - val_acc: 0.9958
Epoch 40/50
20508/20508 [==============================] - 103s 5ms/sample - loss: 0.1613 - acc: 0.9855 - val_loss: 0.1388 - val_acc: 0.9944
Epoch 41/50
20508/20508 [==============================] - 102s 5ms/sample - loss: 0.1621 - acc: 0.9846 - val_loss: 0.1387 - val_acc: 0.9938
Epoch 42/50
20508/20508 [==============================] - 104s 5ms/sample - loss: 0.1549 - acc: 0.9859 - val_loss: 0.1359 - val_acc: 0.9941
Epoch 43/50
20508/20508 [==============================] - 102s 5ms/sample - loss: 0.1560 - acc: 0.9865 - val_loss: 0.1383 - val_acc: 0.9943
Epoch 44/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1576 - acc: 0.9865 - val_loss: 0.1334 - val_acc: 0.9944
Epoch 45/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1637 - acc: 0.9832 - val_loss: 0.1585 - val_acc: 0.9886
Epoch 46/50
20508/20508 [==============================] - 100s 5ms/sample - loss: 0.1548 - acc: 0.9868 - val_loss: 0.1366 - val_acc: 0.9948
Epoch 47/50
20508/20508 [==============================] - 99s 5ms/sample - loss: 0.1577 - acc: 0.9864 - val_loss: 0.1539 - val_acc: 0.9901
Epoch 48/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1593 - acc: 0.9851 - val_loss: 0.1333 - val_acc: 0.9949
Epoch 49/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1496 - acc: 0.9878 - val_loss: 0.1364 - val_acc: 0.9941
Epoch 50/50
20508/20508 [==============================] - 101s 5ms/sample - loss: 0.1549 - acc: 0.9864 - val_loss: 0.1381 - val_acc: 0.9937
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 25, 220, 1)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 13, 110, 32)       320       
_________________________________________________________________
batch_normalization_v1_6 (Ba (None, 13, 110, 32)       128       
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 55, 32)         0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 55, 32)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 3, 28, 64)         18496     
_________________________________________________________________
batch_normalization_v1_7 (Ba (None, 3, 28, 64)         256       
_________________________________________________________________
dropout_8 (Dropout)          (None, 3, 28, 64)         0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 2, 14, 128)        73856     
_________________________________________________________________
batch_normalization_v1_8 (Ba (None, 2, 14, 128)        512       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 1, 7, 128)         0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 1, 7, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 896)               0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 896)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 1024)              918528    
_________________________________________________________________
dropout_11 (Dropout)         (None, 1024)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 5)                 5125      
=================================================================
Total params: 1,017,221
Trainable params: 1,016,773
Non-trainable params: 448
_________________________________________________________________
None
Confusion matrix, without normalization
[[ 475    0    0    0    0]
 [   0 2440   15    0   24]
 [   0   16 3731    0    2]
 [   5    0    0 2188   15]
 [   0    4    2    3 4753]]