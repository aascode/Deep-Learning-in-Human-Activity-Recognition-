Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 244s 5ms/sample - loss: 0.5161 - acc: 0.8607 - val_loss: 0.7708 - val_acc: 0.7490
Epoch 2/50
48636/48636 [==============================] - 245s 5ms/sample - loss: 0.1272 - acc: 0.9826 - val_loss: 0.6511 - val_acc: 0.7673
Epoch 3/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0856 - acc: 0.9930 - val_loss: 0.2043 - val_acc: 0.9442
Epoch 4/50
48636/48636 [==============================] - 264s 5ms/sample - loss: 0.0662 - acc: 0.9959 - val_loss: 0.0939 - val_acc: 0.9868
Epoch 5/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0535 - acc: 0.9977 - val_loss: 0.0487 - val_acc: 0.9974
Epoch 6/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0432 - acc: 0.9988 - val_loss: 0.0410 - val_acc: 0.9980
Epoch 7/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0362 - acc: 0.9991 - val_loss: 0.0329 - val_acc: 0.9988
Epoch 8/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0318 - acc: 0.9990 - val_loss: 0.0268 - val_acc: 0.9994
Epoch 9/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0271 - acc: 0.9991 - val_loss: 0.0264 - val_acc: 0.9986
Epoch 10/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0231 - acc: 0.9993 - val_loss: 0.0218 - val_acc: 0.9992
Epoch 11/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0201 - acc: 0.9995 - val_loss: 0.0180 - val_acc: 0.9995
Epoch 12/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0185 - acc: 0.9993 - val_loss: 0.0159 - val_acc: 0.9996
Epoch 13/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0163 - acc: 0.9995 - val_loss: 0.0162 - val_acc: 0.9995
Epoch 14/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0153 - acc: 0.9995 - val_loss: 0.0137 - val_acc: 0.9998
Epoch 15/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0149 - acc: 0.9994 - val_loss: 0.0138 - val_acc: 0.9995
Epoch 16/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0148 - acc: 0.9995 - val_loss: 0.0128 - val_acc: 0.9997
Epoch 17/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0141 - acc: 0.9994 - val_loss: 0.0147 - val_acc: 0.9993
Epoch 18/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0149 - acc: 0.9992 - val_loss: 0.0144 - val_acc: 0.9993
Epoch 19/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0167 - acc: 0.9990 - val_loss: 0.0147 - val_acc: 0.9991
Epoch 20/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0146 - acc: 0.9994 - val_loss: 0.0127 - val_acc: 0.9997
Epoch 21/50
48636/48636 [==============================] - 261s 5ms/sample - loss: 0.0130 - acc: 0.9996 - val_loss: 0.0115 - val_acc: 0.9998
Epoch 22/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0114 - acc: 0.9997 - val_loss: 0.0106 - val_acc: 0.9994
Epoch 23/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0111 - acc: 0.9996 - val_loss: 0.0133 - val_acc: 0.9985
Epoch 24/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0142 - acc: 0.9990 - val_loss: 0.0124 - val_acc: 0.9994
Epoch 25/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0133 - acc: 0.9993 - val_loss: 0.0120 - val_acc: 0.9995
Epoch 26/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0111 - acc: 0.9998 - val_loss: 0.0107 - val_acc: 0.9996
Epoch 27/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0104 - acc: 0.9995 - val_loss: 0.0115 - val_acc: 0.9988
Epoch 28/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0114 - acc: 0.9992 - val_loss: 0.0110 - val_acc: 0.9993
Epoch 29/50
48636/48636 [==============================] - 261s 5ms/sample - loss: 0.0135 - acc: 0.9993 - val_loss: 0.0117 - val_acc: 0.9998
Epoch 30/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0114 - acc: 0.9997 - val_loss: 0.0104 - val_acc: 0.9998
Epoch 31/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0092 - acc: 0.9999 - val_loss: 0.0092 - val_acc: 0.9996
Epoch 32/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0095 - acc: 0.9995 - val_loss: 0.0093 - val_acc: 0.9997
Epoch 33/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0130 - acc: 0.9990 - val_loss: 0.0113 - val_acc: 0.9998
Epoch 34/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0118 - acc: 0.9995 - val_loss: 0.0119 - val_acc: 0.9993
Epoch 35/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0102 - acc: 0.9999 - val_loss: 0.0093 - val_acc: 0.9998
Epoch 36/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0096 - acc: 0.9996 - val_loss: 0.0103 - val_acc: 0.9995
Epoch 37/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0106 - acc: 0.9994 - val_loss: 0.0110 - val_acc: 0.9994
Epoch 38/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0096 - acc: 0.9997 - val_loss: 0.0098 - val_acc: 0.9993
Epoch 39/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0106 - acc: 0.9994 - val_loss: 0.0104 - val_acc: 0.9995
Epoch 40/50
48636/48636 [==============================] - 265s 5ms/sample - loss: 0.0097 - acc: 0.9996 - val_loss: 0.0096 - val_acc: 0.9998
Epoch 41/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0095 - acc: 0.9997 - val_loss: 0.0090 - val_acc: 0.9997
Epoch 42/50
48636/48636 [==============================] - 279s 6ms/sample - loss: 0.0086 - acc: 0.9998 - val_loss: 0.0083 - val_acc: 0.9998
Epoch 43/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0083 - acc: 0.9998 - val_loss: 0.0106 - val_acc: 0.9981
Epoch 44/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0109 - acc: 0.9991 - val_loss: 0.0110 - val_acc: 0.9995
Epoch 45/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0104 - acc: 0.9996 - val_loss: 0.1047 - val_acc: 0.9763
Epoch 46/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0119 - acc: 0.9994 - val_loss: 0.0108 - val_acc: 0.9995
Epoch 47/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0107 - acc: 0.9996 - val_loss: 0.0134 - val_acc: 0.9989
Epoch 48/50
48636/48636 [==============================] - 262s 5ms/sample - loss: 0.0095 - acc: 0.9997 - val_loss: 0.0190 - val_acc: 0.9967
Epoch 49/50
48636/48636 [==============================] - 263s 5ms/sample - loss: 0.0086 - acc: 0.9999 - val_loss: 0.0078 - val_acc: 0.9998
Epoch 50/50
48636/48636 [==============================] - 264s 5ms/sample - loss: 0.0085 - acc: 0.9996 - val_loss: 0.0092 - val_acc: 0.9992
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 100, 100, 1)  0                                            
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 50, 50, 32)   320         input_37[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v1_35 (Batc (None, 50, 50, 32)   128         conv2d_22[0][0]                  
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 25, 25, 32)   0           batch_normalization_v1_35[0][0]  
__________________________________________________________________________________________________
dropout_64 (Dropout)            (None, 25, 25, 32)   0           max_pooling2d[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 13, 13, 64)   18496       dropout_64[0][0]                 
__________________________________________________________________________________________________
batch_normalization_v1_36 (Batc (None, 13, 13, 64)   256         conv2d_23[0][0]                  
__________________________________________________________________________________________________
input_33 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_35 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_v1_36[0][0]  
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 10, 11, 16)   160         input_33[0][0]                   
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 10, 11, 16)   160         input_34[0][0]                   
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 10, 11, 16)   160         input_35[0][0]                   
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 10, 11, 16)   160         input_36[0][0]                   
__________________________________________________________________________________________________
dropout_65 (Dropout)            (None, 6, 6, 64)     0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_v1_27 (Batc (None, 10, 11, 16)   64          conv2d_18[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_29 (Batc (None, 10, 11, 16)   64          conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_31 (Batc (None, 10, 11, 16)   64          conv2d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_33 (Batc (None, 10, 11, 16)   64          conv2d_21[0][0]                  
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 3, 3, 128)    73856       dropout_65[0][0]                 
__________________________________________________________________________________________________
dropout_52 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_27[0][0]  
__________________________________________________________________________________________________
dropout_55 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_29[0][0]  
__________________________________________________________________________________________________
dropout_58 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_31[0][0]  
__________________________________________________________________________________________________
dropout_61 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_33[0][0]  
__________________________________________________________________________________________________
batch_normalization_v1_37 (Batc (None, 3, 3, 128)    512         conv2d_24[0][0]                  
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 1760)         0           dropout_52[0][0]                 
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 1760)         0           dropout_55[0][0]                 
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 1760)         0           dropout_58[0][0]                 
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 1760)         0           dropout_61[0][0]                 
__________________________________________________________________________________________________
dropout_66 (Dropout)            (None, 3, 3, 128)    0           batch_normalization_v1_37[0][0]  
__________________________________________________________________________________________________
dropout_53 (Dropout)            (None, 1760)         0           flatten_28[0][0]                 
__________________________________________________________________________________________________
dropout_56 (Dropout)            (None, 1760)         0           flatten_29[0][0]                 
__________________________________________________________________________________________________
dropout_59 (Dropout)            (None, 1760)         0           flatten_30[0][0]                 
__________________________________________________________________________________________________
dropout_62 (Dropout)            (None, 1760)         0           flatten_31[0][0]                 
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 1152)         0           dropout_66[0][0]                 
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 128)          225408      dropout_53[0][0]                 
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 128)          225408      dropout_56[0][0]                 
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 128)          225408      dropout_59[0][0]                 
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 128)          225408      dropout_62[0][0]                 
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 256)          295168      flatten_32[0][0]                 
__________________________________________________________________________________________________
batch_normalization_v1_28 (Batc (None, 128)          512         dense_102[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_30 (Batc (None, 128)          512         dense_104[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_32 (Batc (None, 128)          512         dense_106[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_34 (Batc (None, 128)          512         dense_108[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_38 (Batc (None, 256)          1024        dense_110[0][0]                  
__________________________________________________________________________________________________
dropout_54 (Dropout)            (None, 128)          0           batch_normalization_v1_28[0][0]  
__________________________________________________________________________________________________
dropout_57 (Dropout)            (None, 128)          0           batch_normalization_v1_30[0][0]  
__________________________________________________________________________________________________
dropout_60 (Dropout)            (None, 128)          0           batch_normalization_v1_32[0][0]  
__________________________________________________________________________________________________
dropout_63 (Dropout)            (None, 128)          0           batch_normalization_v1_34[0][0]  
__________________________________________________________________________________________________
dropout_67 (Dropout)            (None, 256)          0           batch_normalization_v1_38[0][0]  
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 8)            1032        dropout_54[0][0]                 
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 8)            1032        dropout_57[0][0]                 
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 8)            1032        dropout_60[0][0]                 
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 8)            1032        dropout_63[0][0]                 
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 8)            2056        dropout_67[0][0]                 
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 40)           0           dense_103[0][0]                  
                                                                 dense_105[0][0]                  
                                                                 dense_107[0][0]                  
                                                                 dense_109[0][0]                  
                                                                 dense_111[0][0]                  
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 8)            328         concatenate[0][0]                
==================================================================================================
Total params: 1,300,848
Trainable params: 1,298,736
Non-trainable params: 2,112
__________________________________________________________________________________________________
None
Confusion matrix, without normalization
[[1791    0    0    0    0    0    0    0]
 [   0 1730    0    0    0    0    0    0]
 [   0    0 1615    0    0    0    7    0]
 [   0    0    2  268    0    0    0    0]
 [   0    0    1    0  937    0    0    0]
 [   0    0    0    0    0 2014    0    0]
 [   0    0    0    0    0    0 1976    0]
 [   0    0    0    0    0    0    0 1818]]