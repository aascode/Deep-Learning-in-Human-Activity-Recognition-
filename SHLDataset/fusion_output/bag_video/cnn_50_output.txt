Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 237s 5ms/sample - loss: 1.0192 - acc: 0.6880 - val_loss: 2.0265 - val_acc: 0.4219
Epoch 2/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.2960 - acc: 0.9310 - val_loss: 1.6921 - val_acc: 0.5726
Epoch 3/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.1647 - acc: 0.9713 - val_loss: 0.9135 - val_acc: 0.7241
Epoch 4/50
48636/48636 [==============================] - 253s 5ms/sample - loss: 0.1188 - acc: 0.9836 - val_loss: 0.1213 - val_acc: 0.9841
Epoch 5/50
48636/48636 [==============================] - 240s 5ms/sample - loss: 0.0937 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9967
Epoch 6/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0770 - acc: 0.9934 - val_loss: 0.0561 - val_acc: 0.9983
Epoch 7/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0675 - acc: 0.9944 - val_loss: 0.0523 - val_acc: 0.9981
Epoch 8/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0579 - acc: 0.9965 - val_loss: 0.0456 - val_acc: 0.9988
Epoch 9/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0512 - acc: 0.9970 - val_loss: 0.0426 - val_acc: 0.9983
Epoch 10/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0452 - acc: 0.9975 - val_loss: 0.0378 - val_acc: 0.9989
Epoch 11/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0411 - acc: 0.9974 - val_loss: 0.0327 - val_acc: 0.9996
Epoch 12/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0367 - acc: 0.9980 - val_loss: 0.0296 - val_acc: 0.9993
Epoch 13/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0344 - acc: 0.9976 - val_loss: 0.0290 - val_acc: 0.9992
Epoch 14/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0319 - acc: 0.9979 - val_loss: 0.0306 - val_acc: 0.9986
Epoch 15/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0296 - acc: 0.9980 - val_loss: 0.0250 - val_acc: 0.9991
Epoch 16/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0278 - acc: 0.9980 - val_loss: 0.0247 - val_acc: 0.9990
Epoch 17/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0264 - acc: 0.9983 - val_loss: 0.0222 - val_acc: 0.9996
Epoch 18/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0243 - acc: 0.9985 - val_loss: 0.0200 - val_acc: 0.9998
Epoch 19/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0238 - acc: 0.9983 - val_loss: 0.0211 - val_acc: 0.9995
Epoch 20/50
48636/48636 [==============================] - 229s 5ms/sample - loss: 0.0228 - acc: 0.9985 - val_loss: 0.0200 - val_acc: 0.9993
Epoch 21/50
48636/48636 [==============================] - 228s 5ms/sample - loss: 0.0229 - acc: 0.9986 - val_loss: 0.0200 - val_acc: 0.9990
Epoch 22/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0217 - acc: 0.9986 - val_loss: 0.0178 - val_acc: 0.9997
Epoch 23/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0231 - acc: 0.9981 - val_loss: 0.0195 - val_acc: 0.9991
Epoch 24/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0210 - acc: 0.9988 - val_loss: 0.0171 - val_acc: 0.9997
Epoch 25/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0191 - acc: 0.9991 - val_loss: 0.0160 - val_acc: 0.9998
Epoch 26/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0217 - acc: 0.9982 - val_loss: 0.0191 - val_acc: 0.9995
Epoch 27/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0205 - acc: 0.9985 - val_loss: 0.0163 - val_acc: 0.9998
Epoch 28/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0191 - acc: 0.9990 - val_loss: 0.0181 - val_acc: 0.9991
Epoch 29/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0206 - acc: 0.9984 - val_loss: 0.0165 - val_acc: 0.9996
Epoch 30/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.0192 - acc: 0.9987 - val_loss: 0.0161 - val_acc: 0.9997
Epoch 31/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0203 - acc: 0.9985 - val_loss: 0.0309 - val_acc: 0.9940
Epoch 32/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0190 - acc: 0.9987 - val_loss: 0.0156 - val_acc: 0.9998
Epoch 33/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0181 - acc: 0.9990 - val_loss: 0.0153 - val_acc: 0.9998
Epoch 34/50
48636/48636 [==============================] - 234s 5ms/sample - loss: 0.0171 - acc: 0.9991 - val_loss: 0.0145 - val_acc: 0.9999
Epoch 35/50
48636/48636 [==============================] - 234s 5ms/sample - loss: 0.0166 - acc: 0.9990 - val_loss: 0.0155 - val_acc: 0.9996
Epoch 36/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0196 - acc: 0.9982 - val_loss: 0.0210 - val_acc: 0.9978
Epoch 37/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.0201 - acc: 0.9983 - val_loss: 0.0159 - val_acc: 0.9998
Epoch 38/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0181 - acc: 0.9988 - val_loss: 0.0148 - val_acc: 0.9999
Epoch 39/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.0183 - acc: 0.9986 - val_loss: 0.0152 - val_acc: 0.9998
Epoch 40/50
48636/48636 [==============================] - 233s 5ms/sample - loss: 0.0176 - acc: 0.9989 - val_loss: 0.0148 - val_acc: 0.9998
Epoch 41/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0181 - acc: 0.9988 - val_loss: 0.0148 - val_acc: 0.9998
Epoch 42/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0174 - acc: 0.9990 - val_loss: 0.0146 - val_acc: 0.9998
Epoch 43/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0180 - acc: 0.9985 - val_loss: 0.0146 - val_acc: 0.9998
Epoch 44/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0181 - acc: 0.9988 - val_loss: 0.0155 - val_acc: 0.9998
Epoch 45/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0164 - acc: 0.9992 - val_loss: 0.0138 - val_acc: 0.9999
Epoch 46/50
48636/48636 [==============================] - 231s 5ms/sample - loss: 0.0150 - acc: 0.9994 - val_loss: 0.0130 - val_acc: 0.9999
Epoch 47/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0148 - acc: 0.9991 - val_loss: 0.0309 - val_acc: 0.9956
Epoch 48/50
48636/48636 [==============================] - 232s 5ms/sample - loss: 0.0162 - acc: 0.9990 - val_loss: 0.0145 - val_acc: 0.9995
Epoch 49/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0174 - acc: 0.9987 - val_loss: 0.0182 - val_acc: 0.9984
Epoch 50/50
48636/48636 [==============================] - 230s 5ms/sample - loss: 0.0181 - acc: 0.9988 - val_loss: 0.0148 - val_acc: 0.9997
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 100, 100, 1)  0                                            
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 50, 50, 32)   320         input_41[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v1_47 (Batc (None, 50, 50, 32)   128         conv2d_30[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 25, 25, 32)   0           batch_normalization_v1_47[0][0]  
__________________________________________________________________________________________________
dropout_78 (Dropout)            (None, 25, 25, 32)   0           max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 13, 13, 64)   18496       dropout_78[0][0]                 
__________________________________________________________________________________________________
batch_normalization_v1_48 (Batc (None, 13, 13, 64)   256         conv2d_31[0][0]                  
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_v1_48[0][0]  
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 10, 11, 16)   160         input_40[0][0]                   
__________________________________________________________________________________________________
dropout_79 (Dropout)            (None, 6, 6, 64)     0           max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
batch_normalization_v1_45 (Batc (None, 10, 11, 16)   64          conv2d_29[0][0]                  
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 3, 3, 128)    73856       dropout_79[0][0]                 
__________________________________________________________________________________________________
dropout_75 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_45[0][0]  
__________________________________________________________________________________________________
batch_normalization_v1_49 (Batc (None, 3, 3, 128)    512         conv2d_32[0][0]                  
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 1760)         0           dropout_75[0][0]                 
__________________________________________________________________________________________________
dropout_80 (Dropout)            (None, 3, 3, 128)    0           batch_normalization_v1_49[0][0]  
__________________________________________________________________________________________________
dropout_76 (Dropout)            (None, 1760)         0           flatten_35[0][0]                 
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 1152)         0           dropout_80[0][0]                 
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 128)          225408      dropout_76[0][0]                 
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 256)          295168      flatten_36[0][0]                 
__________________________________________________________________________________________________
batch_normalization_v1_46 (Batc (None, 128)          512         dense_118[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_50 (Batc (None, 256)          1024        dense_120[0][0]                  
__________________________________________________________________________________________________
dropout_77 (Dropout)            (None, 128)          0           batch_normalization_v1_46[0][0]  
__________________________________________________________________________________________________
dropout_81 (Dropout)            (None, 256)          0           batch_normalization_v1_50[0][0]  
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 8)            1032        dropout_77[0][0]                 
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 8)            2056        dropout_81[0][0]                 
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 16)           0           dense_119[0][0]                  
                                                                 dense_121[0][0]                  
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 8)            136         concatenate_2[0][0]              
==================================================================================================
Total params: 619,128
Trainable params: 617,880
Non-trainable params: 1,248
__________________________________________________________________________________________________
None
Confusion matrix, without normalization
[[1790    1    0    0    0    0    0    0]
 [   0 1729    1    0    0    0    0    0]
 [   0    0 1622    0    0    0    0    0]
 [   0    0    1  269    0    0    0    0]
 [   0    0    1    0  937    0    0    0]
 [   0    0    0    0    0 2014    0    0]
 [   0    0    0    0    0    0 1976    0]
 [   0    0    0    0    0    0    0 1818]]