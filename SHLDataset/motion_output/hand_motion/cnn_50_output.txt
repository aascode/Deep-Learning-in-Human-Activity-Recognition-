Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 22s 448us/sample - loss: 1.7713 - acc: 0.3807 - val_loss: 1.5560 - val_acc: 0.4288
Epoch 2/50
48636/48636 [==============================] - 26s 525us/sample - loss: 1.1802 - acc: 0.5694 - val_loss: 1.5000 - val_acc: 0.3856
Epoch 3/50
48636/48636 [==============================] - 24s 491us/sample - loss: 0.9392 - acc: 0.6612 - val_loss: 1.3359 - val_acc: 0.4862
Epoch 4/50
48636/48636 [==============================] - 23s 483us/sample - loss: 0.8258 - acc: 0.7062 - val_loss: 1.4994 - val_acc: 0.3861
Epoch 5/50
48636/48636 [==============================] - 23s 482us/sample - loss: 0.7561 - acc: 0.7318 - val_loss: 1.4181 - val_acc: 0.4387- ETA: 17s - loss: 0.7747 - acc: 0.7243
Epoch 6/50
48636/48636 [==============================] - 24s 488us/sample - loss: 0.7141 - acc: 0.7495 - val_loss: 1.4428 - val_acc: 0.4339- ETA: 10s - loss: 0.7245 - acc: 0.7460
Epoch 7/50
48636/48636 [==============================] - 24s 491us/sample - loss: 0.6795 - acc: 0.7645 - val_loss: 1.2921 - val_acc: 0.5154
Epoch 8/50
48636/48636 [==============================] - 23s 482us/sample - loss: 0.6523 - acc: 0.7725 - val_loss: 1.3271 - val_acc: 0.5007
Epoch 9/50
48636/48636 [==============================] - 24s 488us/sample - loss: 0.6212 - acc: 0.7844 - val_loss: 1.3024 - val_acc: 0.4984
Epoch 10/50
48636/48636 [==============================] - 24s 494us/sample - loss: 0.6060 - acc: 0.7910 - val_loss: 1.2959 - val_acc: 0.4996
Epoch 11/50
48636/48636 [==============================] - 24s 496us/sample - loss: 0.5837 - acc: 0.7995 - val_loss: 1.5429 - val_acc: 0.4243
Epoch 12/50
48636/48636 [==============================] - 24s 498us/sample - loss: 0.5701 - acc: 0.8050 - val_loss: 1.3460 - val_acc: 0.4754
Epoch 13/50
48636/48636 [==============================] - 24s 494us/sample - loss: 0.5554 - acc: 0.8074 - val_loss: 1.3213 - val_acc: 0.4802
Epoch 14/50
48636/48636 [==============================] - 24s 492us/sample - loss: 0.5406 - acc: 0.8160 - val_loss: 1.1900 - val_acc: 0.5346- ETA: 10s - loss: 0.5462 - acc: 0.8148
Epoch 15/50
48636/48636 [==============================] - 24s 504us/sample - loss: 0.5278 - acc: 0.8196 - val_loss: 1.4828 - val_acc: 0.4359
Epoch 16/50
48636/48636 [==============================] - 24s 501us/sample - loss: 0.5174 - acc: 0.8224 - val_loss: 1.2637 - val_acc: 0.5053
Epoch 17/50
48636/48636 [==============================] - 24s 494us/sample - loss: 0.5126 - acc: 0.8236 - val_loss: 1.4244 - val_acc: 0.4482
Epoch 18/50
48636/48636 [==============================] - 24s 497us/sample - loss: 0.4969 - acc: 0.8314 - val_loss: 1.2823 - val_acc: 0.5109
Epoch 19/50
48636/48636 [==============================] - 24s 503us/sample - loss: 0.4936 - acc: 0.8313 - val_loss: 1.3669 - val_acc: 0.4659
Epoch 20/50
48636/48636 [==============================] - 24s 497us/sample - loss: 0.4905 - acc: 0.8337 - val_loss: 1.2107 - val_acc: 0.5266
Epoch 21/50
48636/48636 [==============================] - 24s 498us/sample - loss: 0.4791 - acc: 0.8371 - val_loss: 1.2724 - val_acc: 0.5260
Epoch 22/50
48636/48636 [==============================] - 24s 503us/sample - loss: 0.4808 - acc: 0.8359 - val_loss: 1.2571 - val_acc: 0.5144
Epoch 23/50
48636/48636 [==============================] - 25s 507us/sample - loss: 0.4660 - acc: 0.8397 - val_loss: 1.4720 - val_acc: 0.4358
Epoch 24/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.4636 - acc: 0.8403 - val_loss: 1.4048 - val_acc: 0.4648
Epoch 25/50
48636/48636 [==============================] - 25s 515us/sample - loss: 0.4639 - acc: 0.8403 - val_loss: 1.3400 - val_acc: 0.4947
Epoch 26/50
48636/48636 [==============================] - 25s 506us/sample - loss: 0.4545 - acc: 0.8438 - val_loss: 1.1593 - val_acc: 0.5732
Epoch 27/50
48636/48636 [==============================] - 24s 500us/sample - loss: 0.4500 - acc: 0.8456 - val_loss: 1.4007 - val_acc: 0.4737- ETA: 11s - loss: 0.4564 - acc: 0.8440
Epoch 28/50
48636/48636 [==============================] - 24s 498us/sample - loss: 0.4456 - acc: 0.8489 - val_loss: 1.8209 - val_acc: 0.3805
Epoch 29/50
48636/48636 [==============================] - 24s 499us/sample - loss: 0.4464 - acc: 0.8485 - val_loss: 1.2483 - val_acc: 0.5345
Epoch 30/50
48636/48636 [==============================] - 24s 500us/sample - loss: 0.4379 - acc: 0.8512 - val_loss: 1.5155 - val_acc: 0.4458
Epoch 31/50
48636/48636 [==============================] - 24s 498us/sample - loss: 0.4366 - acc: 0.8526 - val_loss: 1.1878 - val_acc: 0.5628
Epoch 32/50
48636/48636 [==============================] - 24s 499us/sample - loss: 0.4388 - acc: 0.8513 - val_loss: 1.4631 - val_acc: 0.4453
Epoch 33/50
48636/48636 [==============================] - 24s 501us/sample - loss: 0.4290 - acc: 0.8518 - val_loss: 1.2507 - val_acc: 0.5295
Epoch 34/50
48636/48636 [==============================] - 25s 508us/sample - loss: 0.4259 - acc: 0.8545 - val_loss: 1.4128 - val_acc: 0.4784- ETA: 5s - loss: 0.4281 - acc: 0.8540
Epoch 35/50
48636/48636 [==============================] - 24s 497us/sample - loss: 0.4264 - acc: 0.8546 - val_loss: 1.1662 - val_acc: 0.5694
Epoch 36/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.4250 - acc: 0.8552 - val_loss: 1.1399 - val_acc: 0.5873
Epoch 37/50
48636/48636 [==============================] - 25s 510us/sample - loss: 0.4225 - acc: 0.8555 - val_loss: 1.1554 - val_acc: 0.5751
Epoch 38/50
48636/48636 [==============================] - 25s 505us/sample - loss: 0.4168 - acc: 0.8568 - val_loss: 1.2270 - val_acc: 0.5389
Epoch 39/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.4134 - acc: 0.8605 - val_loss: 1.2353 - val_acc: 0.5462
Epoch 40/50
48636/48636 [==============================] - 25s 511us/sample - loss: 0.4144 - acc: 0.8589 - val_loss: 1.2792 - val_acc: 0.5311
Epoch 41/50
48636/48636 [==============================] - 25s 512us/sample - loss: 0.4110 - acc: 0.8606 - val_loss: 1.3324 - val_acc: 0.5100
Epoch 42/50
48636/48636 [==============================] - 25s 508us/sample - loss: 0.4080 - acc: 0.8610 - val_loss: 1.5835 - val_acc: 0.4328
Epoch 43/50
48636/48636 [==============================] - 25s 505us/sample - loss: 0.4057 - acc: 0.8629 - val_loss: 1.8328 - val_acc: 0.3905
Epoch 44/50
48636/48636 [==============================] - 24s 501us/sample - loss: 0.4099 - acc: 0.8598 - val_loss: 1.4996 - val_acc: 0.4645
Epoch 45/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.3989 - acc: 0.8643 - val_loss: 1.4045 - val_acc: 0.4946
Epoch 46/50
48636/48636 [==============================] - 24s 497us/sample - loss: 0.3993 - acc: 0.8622 - val_loss: 1.2869 - val_acc: 0.5418
Epoch 47/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.4029 - acc: 0.8630 - val_loss: 1.1794 - val_acc: 0.5669
Epoch 48/50
48636/48636 [==============================] - 25s 511us/sample - loss: 0.3934 - acc: 0.8662 - val_loss: 1.3323 - val_acc: 0.5228
Epoch 49/50
48636/48636 [==============================] - 25s 504us/sample - loss: 0.3939 - acc: 0.8645 - val_loss: 1.3129 - val_acc: 0.5242
Epoch 50/50
48636/48636 [==============================] - 24s 502us/sample - loss: 0.3956 - acc: 0.8644 - val_loss: 1.5126 - val_acc: 0.4689
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        (None, 20, 22, 1)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 11, 32)        320       
_________________________________________________________________
batch_normalization_v1_6 (Ba (None, 10, 11, 32)        128       
_________________________________________________________________
dropout_22 (Dropout)         (None, 10, 11, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 5, 6, 32)          9248      
_________________________________________________________________
batch_normalization_v1_7 (Ba (None, 5, 6, 32)          128       
_________________________________________________________________
dropout_23 (Dropout)         (None, 5, 6, 32)          0         
_________________________________________________________________
flatten_19 (Flatten)         (None, 960)               0         
_________________________________________________________________
dropout_24 (Dropout)         (None, 960)               0         
_________________________________________________________________
dense_81 (Dense)             (None, 128)               123008    
_________________________________________________________________
batch_normalization_v1_8 (Ba (None, 128)               512       
_________________________________________________________________
dropout_25 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_82 (Dense)             (None, 8)                 1032      
=================================================================
Total params: 134,376
Trainable params: 133,992
Non-trainable params: 384
_________________________________________________________________
None
Confusion matrix, without normalization
[[ 526  305  707    0   10  112   96   35]
 [   3 1137  487    0   11    3   89    0]
 [   1    2 1619    0    0    0    0    0]
 [   0    0   89  177    4    0    0    0]
 [   1    0  437    0  500    0    0    0]
 [  13   18 1379    0    1  569   34    0]
 [  24  231  913    0   20   10  778    0]
 [ 250  212  782    0   24   60   95  395]]