Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 33s 688us/sample - loss: 0.4053 - acc: 0.8683 - val_loss: 0.1051 - val_acc: 0.9696
Epoch 2/50
48636/48636 [==============================] - 34s 690us/sample - loss: 0.0997 - acc: 0.9690 - val_loss: 0.0546 - val_acc: 0.9831
Epoch 3/50
48636/48636 [==============================] - 34s 695us/sample - loss: 0.0692 - acc: 0.9786 - val_loss: 0.0441 - val_acc: 0.9884
Epoch 4/50
48636/48636 [==============================] - 35s 710us/sample - loss: 0.0559 - acc: 0.9822 - val_loss: 0.0380 - val_acc: 0.9873
Epoch 5/50
48636/48636 [==============================] - 35s 728us/sample - loss: 0.0440 - acc: 0.9859 - val_loss: 0.0313 - val_acc: 0.9905- ETA: 17s - loss: 0.0468 - acc: 0.9845
Epoch 6/50
48636/48636 [==============================] - 36s 742us/sample - loss: 0.0382 - acc: 0.9878 - val_loss: 0.0302 - val_acc: 0.9908- ETA: 26s - loss: 0.0414 - acc: 0.9866 - ETA: 22s - loss: 0.0400 - acc: 0.9871
Epoch 7/50
48636/48636 [==============================] - 37s 770us/sample - loss: 0.0348 - acc: 0.9894 - val_loss: 0.0270 - val_acc: 0.9924
Epoch 8/50
48636/48636 [==============================] - 38s 783us/sample - loss: 0.0316 - acc: 0.9906 - val_loss: 0.0222 - val_acc: 0.9938
Epoch 9/50
48636/48636 [==============================] - 40s 827us/sample - loss: 0.0281 - acc: 0.9906 - val_loss: 0.0215 - val_acc: 0.9944
Epoch 10/50
48636/48636 [==============================] - 41s 840us/sample - loss: 0.0285 - acc: 0.9909 - val_loss: 0.0232 - val_acc: 0.9924
Epoch 11/50
48636/48636 [==============================] - 39s 804us/sample - loss: 0.0251 - acc: 0.9924 - val_loss: 0.0179 - val_acc: 0.9956
Epoch 12/50
48636/48636 [==============================] - 40s 813us/sample - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0231 - val_acc: 0.9932
Epoch 13/50
48636/48636 [==============================] - 40s 826us/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 0.0210 - val_acc: 0.9942
Epoch 14/50
48636/48636 [==============================] - 39s 804us/sample - loss: 0.0211 - acc: 0.9934 - val_loss: 0.0199 - val_acc: 0.9944/48636 [======================>.......] - ETA: 7s - loss: 0.0215 - acc: 0.9931
Epoch 15/50
48636/48636 [==============================] - 39s 811us/sample - loss: 0.0183 - acc: 0.9944 - val_loss: 0.0235 - val_acc: 0.9926
Epoch 16/50
48636/48636 [==============================] - 40s 827us/sample - loss: 0.0215 - acc: 0.9934 - val_loss: 0.0251 - val_acc: 0.9937
Epoch 17/50
48636/48636 [==============================] - 39s 793us/sample - loss: 0.0169 - acc: 0.9946 - val_loss: 0.0210 - val_acc: 0.9949
Epoch 18/50
48636/48636 [==============================] - 40s 815us/sample - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0193 - val_acc: 0.9951
Epoch 19/50
48636/48636 [==============================] - 41s 834us/sample - loss: 0.0166 - acc: 0.9944 - val_loss: 0.0208 - val_acc: 0.9954
Epoch 20/50
48636/48636 [==============================] - 39s 812us/sample - loss: 0.0164 - acc: 0.9948 - val_loss: 0.0266 - val_acc: 0.9946
Epoch 21/50
48636/48636 [==============================] - 40s 822us/sample - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0228 - val_acc: 0.9951
Epoch 22/50
48636/48636 [==============================] - 41s 851us/sample - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0181 - val_acc: 0.9950
Epoch 23/50
48636/48636 [==============================] - 40s 814us/sample - loss: 0.0156 - acc: 0.9952 - val_loss: 0.0185 - val_acc: 0.9954
Epoch 24/50
48636/48636 [==============================] - 41s 838us/sample - loss: 0.0158 - acc: 0.9955 - val_loss: 0.0233 - val_acc: 0.9942
Epoch 25/50
48636/48636 [==============================] - 40s 827us/sample - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0208 - val_acc: 0.9949
Epoch 26/50
48636/48636 [==============================] - 41s 846us/sample - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0194 - val_acc: 0.9955
Epoch 27/50
48636/48636 [==============================] - 40s 816us/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0203 - val_acc: 0.9957
Epoch 28/50
48636/48636 [==============================] - 40s 832us/sample - loss: 0.0116 - acc: 0.9966 - val_loss: 0.0191 - val_acc: 0.9956
Epoch 29/50
48636/48636 [==============================] - 40s 816us/sample - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0210 - val_acc: 0.9955
Epoch 30/50
48636/48636 [==============================] - 39s 808us/sample - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0199 - val_acc: 0.9950
Epoch 31/50
48636/48636 [==============================] - 41s 834us/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0173 - val_acc: 0.9955
Epoch 32/50
48636/48636 [==============================] - 39s 811us/sample - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0185 - val_acc: 0.9948
Epoch 33/50
48636/48636 [==============================] - 39s 806us/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.0187 - val_acc: 0.9961
Epoch 34/50
48636/48636 [==============================] - 42s 855us/sample - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0218 - val_acc: 0.9956
Epoch 35/50
48636/48636 [==============================] - 41s 839us/sample - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0180 - val_acc: 0.9961
Epoch 36/50
48636/48636 [==============================] - 40s 812us/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0171 - val_acc: 0.9961
Epoch 37/50
48636/48636 [==============================] - 41s 840us/sample - loss: 0.0110 - acc: 0.9969 - val_loss: 0.0175 - val_acc: 0.9964
Epoch 38/50
48636/48636 [==============================] - 40s 823us/sample - loss: 0.0104 - acc: 0.9971 - val_loss: 0.0218 - val_acc: 0.9954
Epoch 39/50
48636/48636 [==============================] - 40s 832us/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0176 - val_acc: 0.9964
Epoch 40/50
48636/48636 [==============================] - 41s 840us/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0168 - val_acc: 0.9963
Epoch 41/50
48636/48636 [==============================] - 40s 829us/sample - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0189 - val_acc: 0.9962
Epoch 42/50
48636/48636 [==============================] - 42s 854us/sample - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0171 - val_acc: 0.9963
Epoch 43/50
48636/48636 [==============================] - 40s 819us/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0163 - val_acc: 0.9957
Epoch 44/50
48636/48636 [==============================] - 41s 837us/sample - loss: 0.0083 - acc: 0.9977 - val_loss: 0.0204 - val_acc: 0.9965
Epoch 45/50
48636/48636 [==============================] - 41s 841us/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0182 - val_acc: 0.9957
Epoch 46/50
48636/48636 [==============================] - 40s 830us/sample - loss: 0.0091 - acc: 0.9977 - val_loss: 0.0211 - val_acc: 0.9968
Epoch 47/50
48636/48636 [==============================] - 41s 838us/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.0150 - val_acc: 0.9969
Epoch 48/50
48636/48636 [==============================] - 40s 816us/sample - loss: 0.0075 - acc: 0.9977 - val_loss: 0.0200 - val_acc: 0.9961
Epoch 49/50
48636/48636 [==============================] - 41s 850us/sample - loss: 0.0102 - acc: 0.9974 - val_loss: 0.0190 - val_acc: 0.9964
Epoch 50/50
48636/48636 [==============================] - 41s 848us/sample - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0203 - val_acc: 0.9957
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_63 (InputLayer)        (None, 20, 88, 1)         0         
_________________________________________________________________
conv2d_55 (Conv2D)           (None, 10, 44, 16)        160       
_________________________________________________________________
batch_normalization_v1_86 (B (None, 10, 44, 16)        64        
_________________________________________________________________
dropout_144 (Dropout)        (None, 10, 44, 16)        0         
_________________________________________________________________
flatten_57 (Flatten)         (None, 7040)              0         
_________________________________________________________________
dropout_145 (Dropout)        (None, 7040)              0         
_________________________________________________________________
dense_166 (Dense)            (None, 128)               901248    
_________________________________________________________________
dropout_146 (Dropout)        (None, 128)               0         
_________________________________________________________________
dense_167 (Dense)            (None, 8)                 1032      
=================================================================
Total params: 902,504
Trainable params: 902,472
Non-trainable params: 32
_________________________________________________________________
None
Confusion matrix, without normalization
[[1767    0    2    0    2    0    0   20]
 [   6 1724    0    0    0    0    0    0]
 [   0    0 1611    7    0    0    4    0]
 [   0    0    2  268    0    0    0    0]
 [   0    0    3    0  935    0    0    0]
 [   0    0    0    0    0 2014    0    0]
 [   0    0    4    0    0    0 1972    0]
 [   2    0    0    0    0    0    0 1816]]