Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 29s 587us/sample - loss: 0.4280 - acc: 0.8673 - val_loss: 0.1438 - val_acc: 0.9550
Epoch 2/50
48636/48636 [==============================] - 26s 528us/sample - loss: 0.1252 - acc: 0.9617 - val_loss: 0.0900 - val_acc: 0.9696
Epoch 3/50
48636/48636 [==============================] - 26s 540us/sample - loss: 0.0858 - acc: 0.9747 - val_loss: 0.0625 - val_acc: 0.9815
Epoch 4/50
48636/48636 [==============================] - 26s 542us/sample - loss: 0.0663 - acc: 0.9807 - val_loss: 0.0523 - val_acc: 0.9849
Epoch 5/50
48636/48636 [==============================] - 27s 555us/sample - loss: 0.0547 - acc: 0.9831 - val_loss: 0.0444 - val_acc: 0.9858
Epoch 6/50
48636/48636 [==============================] - 28s 568us/sample - loss: 0.0469 - acc: 0.9859 - val_loss: 0.0407 - val_acc: 0.9891
Epoch 7/50
48636/48636 [==============================] - 28s 574us/sample - loss: 0.0397 - acc: 0.9880 - val_loss: 0.0406 - val_acc: 0.9872
Epoch 8/50
48636/48636 [==============================] - 29s 593us/sample - loss: 0.0363 - acc: 0.9889 - val_loss: 0.0372 - val_acc: 0.9882
Epoch 9/50
48636/48636 [==============================] - 29s 598us/sample - loss: 0.0331 - acc: 0.9903 - val_loss: 0.0323 - val_acc: 0.9905
Epoch 10/50
48636/48636 [==============================] - 30s 619us/sample - loss: 0.0294 - acc: 0.9911 - val_loss: 0.0287 - val_acc: 0.9922
Epoch 11/50
48636/48636 [==============================] - 31s 633us/sample - loss: 0.0263 - acc: 0.9922 - val_loss: 0.0282 - val_acc: 0.9922
Epoch 12/50
48636/48636 [==============================] - 32s 654us/sample - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0285 - val_acc: 0.9928
Epoch 13/50
48636/48636 [==============================] - 31s 638us/sample - loss: 0.0232 - acc: 0.9927 - val_loss: 0.0295 - val_acc: 0.9914
Epoch 14/50
48636/48636 [==============================] - 31s 644us/sample - loss: 0.0238 - acc: 0.9928 - val_loss: 0.0314 - val_acc: 0.9905
Epoch 15/50
48636/48636 [==============================] - 31s 633us/sample - loss: 0.0191 - acc: 0.9946 - val_loss: 0.0251 - val_acc: 0.9929
Epoch 16/50
48636/48636 [==============================] - 31s 630us/sample - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0332 - val_acc: 0.9905
Epoch 17/50
48636/48636 [==============================] - 30s 627us/sample - loss: 0.0188 - acc: 0.9946 - val_loss: 0.0239 - val_acc: 0.9937
Epoch 18/50
48636/48636 [==============================] - 31s 628us/sample - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0243 - val_acc: 0.9936
Epoch 19/50
48636/48636 [==============================] - 31s 646us/sample - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0248 - val_acc: 0.9933
Epoch 20/50
48636/48636 [==============================] - 31s 642us/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.0253 - val_acc: 0.9932
Epoch 21/50
48636/48636 [==============================] - 31s 641us/sample - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0245 - val_acc: 0.9947
Epoch 22/50
48636/48636 [==============================] - 31s 638us/sample - loss: 0.0145 - acc: 0.9958 - val_loss: 0.0223 - val_acc: 0.9939
Epoch 23/50
48636/48636 [==============================] - 32s 655us/sample - loss: 0.0164 - acc: 0.9951 - val_loss: 0.0226 - val_acc: 0.9951
Epoch 24/50
48636/48636 [==============================] - 31s 639us/sample - loss: 0.0150 - acc: 0.9958 - val_loss: 0.0247 - val_acc: 0.9937
Epoch 25/50
48636/48636 [==============================] - 31s 639us/sample - loss: 0.0127 - acc: 0.9964 - val_loss: 0.0227 - val_acc: 0.9947
Epoch 26/50
48636/48636 [==============================] - 31s 641us/sample - loss: 0.0132 - acc: 0.9962 - val_loss: 0.0263 - val_acc: 0.9933
Epoch 27/50
48636/48636 [==============================] - 31s 646us/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.0240 - val_acc: 0.9935
Epoch 28/50
48636/48636 [==============================] - 31s 643us/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.0222 - val_acc: 0.9943
Epoch 29/50
48636/48636 [==============================] - 31s 637us/sample - loss: 0.0133 - acc: 0.9962 - val_loss: 0.0238 - val_acc: 0.9936
Epoch 30/50
48636/48636 [==============================] - 32s 653us/sample - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0210 - val_acc: 0.9948
Epoch 31/50
48636/48636 [==============================] - 32s 653us/sample - loss: 0.0111 - acc: 0.9974 - val_loss: 0.0236 - val_acc: 0.9945
Epoch 32/50
48636/48636 [==============================] - 31s 639us/sample - loss: 0.0107 - acc: 0.9969 - val_loss: 0.0222 - val_acc: 0.9945
Epoch 33/50
48636/48636 [==============================] - 32s 660us/sample - loss: 0.0111 - acc: 0.9964 - val_loss: 0.0299 - val_acc: 0.9909
Epoch 34/50
48636/48636 [==============================] - 31s 629us/sample - loss: 0.0117 - acc: 0.9966 - val_loss: 0.0363 - val_acc: 0.9892
Epoch 35/50
48636/48636 [==============================] - 31s 643us/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.0225 - val_acc: 0.9935
Epoch 36/50
48636/48636 [==============================] - 31s 643us/sample - loss: 0.0102 - acc: 0.9971 - val_loss: 0.0217 - val_acc: 0.9947
Epoch 37/50
48636/48636 [==============================] - 31s 634us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.0240 - val_acc: 0.9928
Epoch 38/50
48636/48636 [==============================] - 31s 645us/sample - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0237 - val_acc: 0.9929
Epoch 39/50
48636/48636 [==============================] - 33s 682us/sample - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0214 - val_acc: 0.9944
Epoch 40/50
48636/48636 [==============================] - 32s 653us/sample - loss: 0.0088 - acc: 0.9975 - val_loss: 0.0236 - val_acc: 0.9942
Epoch 41/50
48636/48636 [==============================] - 32s 650us/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0236 - val_acc: 0.9942
Epoch 42/50
48636/48636 [==============================] - 32s 658us/sample - loss: 0.0082 - acc: 0.9980 - val_loss: 0.0231 - val_acc: 0.9941
Epoch 43/50
48636/48636 [==============================] - 32s 654us/sample - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0235 - val_acc: 0.9940
Epoch 44/50
48636/48636 [==============================] - 32s 660us/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0235 - val_acc: 0.9941
Epoch 45/50
48636/48636 [==============================] - 32s 655us/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0230 - val_acc: 0.9935
Epoch 46/50
48636/48636 [==============================] - 32s 657us/sample - loss: 0.0092 - acc: 0.9973 - val_loss: 0.0283 - val_acc: 0.9922
Epoch 47/50
48636/48636 [==============================] - 32s 659us/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.0313 - val_acc: 0.9927
Epoch 48/50
48636/48636 [==============================] - 32s 663us/sample - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0237 - val_acc: 0.9943
Epoch 49/50
48636/48636 [==============================] - 32s 660us/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0231 - val_acc: 0.9942
Epoch 50/50
48636/48636 [==============================] - 33s 687us/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0257 - val_acc: 0.9934
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_49 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 20, 22, 1)    0                                            
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 10, 11, 16)   160         input_47[0][0]                   
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 10, 11, 16)   160         input_48[0][0]                   
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 10, 11, 16)   160         input_49[0][0]                   
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 10, 11, 16)   160         input_50[0][0]                   
__________________________________________________________________________________________________
batch_normalization_v1_62 (Batc (None, 10, 11, 16)   64          conv2d_39[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_64 (Batc (None, 10, 11, 16)   64          conv2d_40[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_66 (Batc (None, 10, 11, 16)   64          conv2d_41[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_68 (Batc (None, 10, 11, 16)   64          conv2d_42[0][0]                  
__________________________________________________________________________________________________
dropout_98 (Dropout)            (None, 10, 11, 16)   0           batch_normalization_v1_62[0][0]  
__________________________________________________________________________________________________
dropout_101 (Dropout)           (None, 10, 11, 16)   0           batch_normalization_v1_64[0][0]  
__________________________________________________________________________________________________
dropout_104 (Dropout)           (None, 10, 11, 16)   0           batch_normalization_v1_66[0][0]  
__________________________________________________________________________________________________
dropout_107 (Dropout)           (None, 10, 11, 16)   0           batch_normalization_v1_68[0][0]  
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 1760)         0           dropout_98[0][0]                 
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 1760)         0           dropout_101[0][0]                
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 1760)         0           dropout_104[0][0]                
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 1760)         0           dropout_107[0][0]                
__________________________________________________________________________________________________
dropout_99 (Dropout)            (None, 1760)         0           flatten_42[0][0]                 
__________________________________________________________________________________________________
dropout_102 (Dropout)           (None, 1760)         0           flatten_43[0][0]                 
__________________________________________________________________________________________________
dropout_105 (Dropout)           (None, 1760)         0           flatten_44[0][0]                 
__________________________________________________________________________________________________
dropout_108 (Dropout)           (None, 1760)         0           flatten_45[0][0]                 
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 128)          225408      dropout_99[0][0]                 
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 128)          225408      dropout_102[0][0]                
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 128)          225408      dropout_105[0][0]                
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 128)          225408      dropout_108[0][0]                
__________________________________________________________________________________________________
batch_normalization_v1_63 (Batc (None, 128)          512         dense_134[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_65 (Batc (None, 128)          512         dense_136[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_67 (Batc (None, 128)          512         dense_138[0][0]                  
__________________________________________________________________________________________________
batch_normalization_v1_69 (Batc (None, 128)          512         dense_140[0][0]                  
__________________________________________________________________________________________________
dropout_100 (Dropout)           (None, 128)          0           batch_normalization_v1_63[0][0]  
__________________________________________________________________________________________________
dropout_103 (Dropout)           (None, 128)          0           batch_normalization_v1_65[0][0]  
__________________________________________________________________________________________________
dropout_106 (Dropout)           (None, 128)          0           batch_normalization_v1_67[0][0]  
__________________________________________________________________________________________________
dropout_109 (Dropout)           (None, 128)          0           batch_normalization_v1_69[0][0]  
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 8)            1032        dropout_100[0][0]                
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 8)            1032        dropout_103[0][0]                
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 8)            1032        dropout_106[0][0]                
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 8)            1032        dropout_109[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 32)           0           dense_135[0][0]                  
                                                                 dense_137[0][0]                  
                                                                 dense_139[0][0]                  
                                                                 dense_141[0][0]                  
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 8)            264         concatenate_4[0][0]              
==================================================================================================
Total params: 908,968
Trainable params: 907,816
Non-trainable params: 1,152
__________________________________________________________________________________________________
None
Confusion matrix, without normalization
[[1764    1    4    0    1    0    0   21]
 [  13 1716    0    0    0    0    1    0]
 [   0    0 1615    1    2    0    4    0]
 [   0    0    9  261    0    0    0    0]
 [   0    0    2    0  934    0    0    2]
 [   0    0    0    0    0 2014    0    0]
 [   0    0    5    0    0    0 1971    0]
 [  14    0    0    0    0    0    0 1804]]