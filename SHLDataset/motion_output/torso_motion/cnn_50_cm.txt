Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 10s 200us/sample - loss: 0.9339 - acc: 0.6703 - val_loss: 0.5792 - val_acc: 0.8017
Epoch 2/50
48636/48636 [==============================] - 8s 174us/sample - loss: 0.5834 - acc: 0.7942 - val_loss: 0.4549 - val_acc: 0.8467
Epoch 3/50
48636/48636 [==============================] - 10s 204us/sample - loss: 0.5097 - acc: 0.8206 - val_loss: 0.4269 - val_acc: 0.8502
Epoch 4/50
48636/48636 [==============================] - 10s 208us/sample - loss: 0.4727 - acc: 0.8345 - val_loss: 0.3778 - val_acc: 0.8655
Epoch 5/50
48636/48636 [==============================] - 10s 208us/sample - loss: 0.4439 - acc: 0.8453 - val_loss: 0.3678 - val_acc: 0.8687
Epoch 6/50
48636/48636 [==============================] - 10s 205us/sample - loss: 0.4209 - acc: 0.8498 - val_loss: 0.3441 - val_acc: 0.8744
Epoch 7/50
48636/48636 [==============================] - 10s 207us/sample - loss: 0.4031 - acc: 0.8591 - val_loss: 0.3372 - val_acc: 0.8813
Epoch 8/50
48636/48636 [==============================] - 10s 210us/sample - loss: 0.3860 - acc: 0.8620 - val_loss: 0.3200 - val_acc: 0.8841
Epoch 9/50
48636/48636 [==============================] - 10s 211us/sample - loss: 0.3788 - acc: 0.8637 - val_loss: 0.3077 - val_acc: 0.8926
Epoch 10/50
48636/48636 [==============================] - 10s 213us/sample - loss: 0.3697 - acc: 0.8681 - val_loss: 0.3017 - val_acc: 0.8914
Epoch 11/50
48636/48636 [==============================] - 11s 218us/sample - loss: 0.3548 - acc: 0.8724 - val_loss: 0.2896 - val_acc: 0.8918
Epoch 12/50
48636/48636 [==============================] - 11s 216us/sample - loss: 0.3506 - acc: 0.8738 - val_loss: 0.2981 - val_acc: 0.8939
Epoch 13/50
48636/48636 [==============================] - 11s 219us/sample - loss: 0.3425 - acc: 0.8777 - val_loss: 0.2870 - val_acc: 0.8936
Epoch 14/50
48636/48636 [==============================] - 11s 220us/sample - loss: 0.3353 - acc: 0.8791 - val_loss: 0.2978 - val_acc: 0.8883
Epoch 15/50
48636/48636 [==============================] - 11s 229us/sample - loss: 0.3329 - acc: 0.8797 - val_loss: 0.2753 - val_acc: 0.9011
Epoch 16/50
48636/48636 [==============================] - 11s 229us/sample - loss: 0.3260 - acc: 0.8818 - val_loss: 0.2783 - val_acc: 0.9007
Epoch 17/50
48636/48636 [==============================] - 11s 232us/sample - loss: 0.3237 - acc: 0.8819 - val_loss: 0.2552 - val_acc: 0.9108
Epoch 18/50
48636/48636 [==============================] - 11s 233us/sample - loss: 0.3202 - acc: 0.8837 - val_loss: 0.3006 - val_acc: 0.8889
Epoch 19/50
48636/48636 [==============================] - 11s 234us/sample - loss: 0.3176 - acc: 0.8867 - val_loss: 0.2544 - val_acc: 0.9132
Epoch 20/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.3078 - acc: 0.8892 - val_loss: 0.2519 - val_acc: 0.9129
Epoch 21/50
48636/48636 [==============================] - 12s 241us/sample - loss: 0.3052 - acc: 0.8892 - val_loss: 0.2535 - val_acc: 0.9138
Epoch 22/50
48636/48636 [==============================] - 12s 240us/sample - loss: 0.3040 - acc: 0.8894 - val_loss: 0.2496 - val_acc: 0.9155
Epoch 23/50
48636/48636 [==============================] - 12s 249us/sample - loss: 0.3001 - acc: 0.8922 - val_loss: 0.2499 - val_acc: 0.9120
Epoch 24/50
48636/48636 [==============================] - 12s 247us/sample - loss: 0.2974 - acc: 0.8916 - val_loss: 0.2370 - val_acc: 0.9192- ETA: 0s - loss: 0.2974 - acc: 0.8920
Epoch 25/50
48636/48636 [==============================] - 12s 247us/sample - loss: 0.2894 - acc: 0.8962 - val_loss: 0.2427 - val_acc: 0.9118
Epoch 26/50
48636/48636 [==============================] - 12s 245us/sample - loss: 0.2901 - acc: 0.8953 - val_loss: 0.2370 - val_acc: 0.9174
Epoch 27/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.2897 - acc: 0.8941 - val_loss: 0.2458 - val_acc: 0.9143
Epoch 28/50
48636/48636 [==============================] - 12s 244us/sample - loss: 0.2899 - acc: 0.8964 - val_loss: 0.2362 - val_acc: 0.9172
Epoch 29/50
48636/48636 [==============================] - 12s 238us/sample - loss: 0.2834 - acc: 0.8962 - val_loss: 0.2437 - val_acc: 0.9136
Epoch 30/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.2812 - acc: 0.8965 - val_loss: 0.2339 - val_acc: 0.9192
Epoch 31/50
48636/48636 [==============================] - 12s 238us/sample - loss: 0.2804 - acc: 0.8993 - val_loss: 0.2528 - val_acc: 0.9094
Epoch 32/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.2803 - acc: 0.8987 - val_loss: 0.2298 - val_acc: 0.9238
Epoch 33/50
48636/48636 [==============================] - 11s 235us/sample - loss: 0.2751 - acc: 0.9007 - val_loss: 0.2399 - val_acc: 0.9146
Epoch 34/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.2740 - acc: 0.9001 - val_loss: 0.2313 - val_acc: 0.9174
Epoch 35/50
48636/48636 [==============================] - 11s 234us/sample - loss: 0.2717 - acc: 0.9011 - val_loss: 0.2326 - val_acc: 0.9224
Epoch 36/50
48636/48636 [==============================] - 11s 234us/sample - loss: 0.2744 - acc: 0.9001 - val_loss: 0.2226 - val_acc: 0.9234
Epoch 37/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.2736 - acc: 0.8994 - val_loss: 0.2323 - val_acc: 0.9184
Epoch 38/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.2663 - acc: 0.9036 - val_loss: 0.2221 - val_acc: 0.9256
Epoch 39/50
48636/48636 [==============================] - 12s 240us/sample - loss: 0.2695 - acc: 0.9026 - val_loss: 0.2626 - val_acc: 0.9070
Epoch 40/50
48636/48636 [==============================] - 12s 244us/sample - loss: 0.2654 - acc: 0.9038 - val_loss: 0.2222 - val_acc: 0.9224
Epoch 41/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.2674 - acc: 0.9034 - val_loss: 0.2159 - val_acc: 0.9296
Epoch 42/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.2628 - acc: 0.9044 - val_loss: 0.2255 - val_acc: 0.9207
Epoch 43/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.2634 - acc: 0.9043 - val_loss: 0.2248 - val_acc: 0.9230
Epoch 44/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.2625 - acc: 0.9038 - val_loss: 0.2101 - val_acc: 0.9301
Epoch 45/50
48636/48636 [==============================] - 12s 240us/sample - loss: 0.2567 - acc: 0.9062 - val_loss: 0.2219 - val_acc: 0.9238
Epoch 46/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.2589 - acc: 0.9067 - val_loss: 0.2161 - val_acc: 0.9261
Epoch 47/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.2531 - acc: 0.9084 - val_loss: 0.2153 - val_acc: 0.9261
Epoch 48/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.2548 - acc: 0.9070 - val_loss: 0.2584 - val_acc: 0.9077
Epoch 49/50
48636/48636 [==============================] - 12s 248us/sample - loss: 0.2721 - acc: 0.9018 - val_loss: 0.2275 - val_acc: 0.9230
Epoch 50/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.2660 - acc: 0.9035 - val_loss: 0.2467 - val_acc: 0.9139
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_30 (InputLayer)        (None, 20, 22, 1)         0         
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 10, 11, 16)        160       
_________________________________________________________________
batch_normalization_v1_23 (B (None, 10, 11, 16)        64        
_________________________________________________________________
dropout_45 (Dropout)         (None, 10, 11, 16)        0         
_________________________________________________________________
flatten_25 (Flatten)         (None, 1760)              0         
_________________________________________________________________
dropout_46 (Dropout)         (None, 1760)              0         
_________________________________________________________________
dense_93 (Dense)             (None, 128)               225408    
_________________________________________________________________
batch_normalization_v1_24 (B (None, 128)               512       
_________________________________________________________________
dropout_47 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_94 (Dense)             (None, 8)                 1032      
=================================================================
Total params: 227,176
Trainable params: 226,888
Non-trainable params: 288
_________________________________________________________________
None
Confusion matrix, without normalization
[[1483   11   12    0    1   31  119  134]
 [  10 1545    1    0    2    0  151   21]
 [   1    1 1581    8    5    0   26    0]
 [   0    0    5  265    0    0    0    0]
 [   6   10    3    0  906    0   13    0]
 [  11    0    1    0    2 1941   47   12]
 [  52   14   30    0    1   53 1792   34]
 [  75    3    4    0    0   38   99 1599]]