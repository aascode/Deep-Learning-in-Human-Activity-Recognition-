Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 25s 509us/sample - loss: 0.9557 - acc: 0.6725 - val_loss: 0.4396 - val_acc: 0.8631
Epoch 2/50
48636/48636 [==============================] - 24s 503us/sample - loss: 0.3999 - acc: 0.8688 - val_loss: 0.2349 - val_acc: 0.9279
Epoch 3/50
48636/48636 [==============================] - 24s 492us/sample - loss: 0.2954 - acc: 0.9057 - val_loss: 0.1925 - val_acc: 0.9432
Epoch 4/50
48636/48636 [==============================] - 24s 489us/sample - loss: 0.2461 - acc: 0.9221 - val_loss: 0.1630 - val_acc: 0.9502- ETA: 16s - loss: 0.2585 - acc: 0.9170
Epoch 5/50
48636/48636 [==============================] - 24s 488us/sample - loss: 0.2177 - acc: 0.9322 - val_loss: 0.1439 - val_acc: 0.9574
Epoch 6/50
48636/48636 [==============================] - 23s 483us/sample - loss: 0.1959 - acc: 0.9387 - val_loss: 0.1349 - val_acc: 0.9591
Epoch 7/50
48636/48636 [==============================] - 23s 480us/sample - loss: 0.1788 - acc: 0.9452 - val_loss: 0.1190 - val_acc: 0.9657
Epoch 8/50
48636/48636 [==============================] - 23s 480us/sample - loss: 0.1677 - acc: 0.9481 - val_loss: 0.1228 - val_acc: 0.9636
Epoch 9/50
48636/48636 [==============================] - 23s 477us/sample - loss: 0.1585 - acc: 0.9516 - val_loss: 0.1108 - val_acc: 0.9660
Epoch 10/50
48636/48636 [==============================] - 23s 479us/sample - loss: 0.1496 - acc: 0.9537 - val_loss: 0.1132 - val_acc: 0.9650
Epoch 11/50
48636/48636 [==============================] - 24s 486us/sample - loss: 0.1419 - acc: 0.9562 - val_loss: 0.1038 - val_acc: 0.9691
Epoch 12/50
48636/48636 [==============================] - 24s 492us/sample - loss: 0.1382 - acc: 0.9568 - val_loss: 0.0950 - val_acc: 0.9711
Epoch 13/50
48636/48636 [==============================] - 24s 486us/sample - loss: 0.1308 - acc: 0.9591 - val_loss: 0.0981 - val_acc: 0.9700
Epoch 14/50
48636/48636 [==============================] - 24s 483us/sample - loss: 0.1280 - acc: 0.9607 - val_loss: 0.0910 - val_acc: 0.9722
Epoch 15/50
48636/48636 [==============================] - 23s 479us/sample - loss: 0.1247 - acc: 0.9608 - val_loss: 0.0836 - val_acc: 0.9754
Epoch 16/50
48636/48636 [==============================] - 23s 479us/sample - loss: 0.1206 - acc: 0.9627 - val_loss: 0.0870 - val_acc: 0.9718
Epoch 17/50
48636/48636 [==============================] - 24s 485us/sample - loss: 0.1191 - acc: 0.9621 - val_loss: 0.0914 - val_acc: 0.9714
Epoch 18/50
48636/48636 [==============================] - 23s 481us/sample - loss: 0.1143 - acc: 0.9638 - val_loss: 0.0847 - val_acc: 0.9720
Epoch 19/50
48636/48636 [==============================] - 23s 480us/sample - loss: 0.1111 - acc: 0.9652 - val_loss: 0.0916 - val_acc: 0.9701
Epoch 20/50
48636/48636 [==============================] - 23s 477us/sample - loss: 0.1112 - acc: 0.9647 - val_loss: 0.0815 - val_acc: 0.9744
Epoch 21/50
48636/48636 [==============================] - 23s 477us/sample - loss: 0.1091 - acc: 0.9659 - val_loss: 0.0777 - val_acc: 0.9762
Epoch 22/50
48636/48636 [==============================] - 23s 476us/sample - loss: 0.1042 - acc: 0.9673 - val_loss: 0.0784 - val_acc: 0.9755
Epoch 23/50
48636/48636 [==============================] - 23s 479us/sample - loss: 0.1040 - acc: 0.9673 - val_loss: 0.0845 - val_acc: 0.9728
Epoch 24/50
48636/48636 [==============================] - 23s 475us/sample - loss: 0.1014 - acc: 0.9678 - val_loss: 0.0752 - val_acc: 0.9778
Epoch 25/50
48636/48636 [==============================] - 23s 478us/sample - loss: 0.1033 - acc: 0.9681 - val_loss: 0.0765 - val_acc: 0.9778
Epoch 26/50
48636/48636 [==============================] - 23s 481us/sample - loss: 0.0994 - acc: 0.9687 - val_loss: 0.0720 - val_acc: 0.9775
Epoch 27/50
48636/48636 [==============================] - 23s 481us/sample - loss: 0.0983 - acc: 0.9697 - val_loss: 0.0731 - val_acc: 0.9778
Epoch 28/50
48636/48636 [==============================] - 23s 475us/sample - loss: 0.0944 - acc: 0.9708 - val_loss: 0.0699 - val_acc: 0.9784
Epoch 29/50
48636/48636 [==============================] - 23s 475us/sample - loss: 0.0942 - acc: 0.9707 - val_loss: 0.0697 - val_acc: 0.9789
Epoch 30/50
48636/48636 [==============================] - 23s 479us/sample - loss: 0.0934 - acc: 0.9703 - val_loss: 0.0749 - val_acc: 0.9770
Epoch 31/50
48636/48636 [==============================] - 23s 478us/sample - loss: 0.0953 - acc: 0.9693 - val_loss: 0.0741 - val_acc: 0.9766
Epoch 32/50
48636/48636 [==============================] - 24s 489us/sample - loss: 0.0918 - acc: 0.9703 - val_loss: 0.0822 - val_acc: 0.9738
Epoch 33/50
48636/48636 [==============================] - 24s 486us/sample - loss: 0.0908 - acc: 0.9715 - val_loss: 0.0736 - val_acc: 0.9775
Epoch 34/50
48636/48636 [==============================] - 24s 484us/sample - loss: 0.0907 - acc: 0.9715 - val_loss: 0.0692 - val_acc: 0.9795
Epoch 35/50
48636/48636 [==============================] - 24s 489us/sample - loss: 0.0880 - acc: 0.9726 - val_loss: 0.0738 - val_acc: 0.9771
Epoch 36/50
48636/48636 [==============================] - 24s 487us/sample - loss: 0.0889 - acc: 0.9712 - val_loss: 0.0664 - val_acc: 0.9797
Epoch 37/50
48636/48636 [==============================] - 25s 505us/sample - loss: 0.0853 - acc: 0.9724 - val_loss: 0.0657 - val_acc: 0.9796
Epoch 38/50
48636/48636 [==============================] - 25s 506us/sample - loss: 0.0872 - acc: 0.9726 - val_loss: 0.0659 - val_acc: 0.9795
Epoch 39/50
48636/48636 [==============================] - 20s 410us/sample - loss: 0.0858 - acc: 0.9733 - val_loss: 0.0659 - val_acc: 0.9796
Epoch 40/50
48636/48636 [==============================] - 20s 410us/sample - loss: 0.0841 - acc: 0.9728 - val_loss: 0.0671 - val_acc: 0.9795
Epoch 41/50
48636/48636 [==============================] - 20s 416us/sample - loss: 0.0829 - acc: 0.9740 - val_loss: 0.0634 - val_acc: 0.9809
Epoch 42/50
48636/48636 [==============================] - 21s 429us/sample - loss: 0.0829 - acc: 0.9739 - val_loss: 0.0668 - val_acc: 0.9809
Epoch 43/50
48636/48636 [==============================] - 21s 433us/sample - loss: 0.0832 - acc: 0.9737 - val_loss: 0.0682 - val_acc: 0.9806
Epoch 44/50
48636/48636 [==============================] - 21s 435us/sample - loss: 0.0818 - acc: 0.9748 - val_loss: 0.0668 - val_acc: 0.9797
Epoch 45/50
48636/48636 [==============================] - 22s 446us/sample - loss: 0.0843 - acc: 0.9738 - val_loss: 0.0689 - val_acc: 0.9806
Epoch 46/50
48636/48636 [==============================] - 22s 453us/sample - loss: 0.0845 - acc: 0.9733 - val_loss: 0.0723 - val_acc: 0.9775
Epoch 47/50
48636/48636 [==============================] - 22s 458us/sample - loss: 0.0795 - acc: 0.9745 - val_loss: 0.0645 - val_acc: 0.9817
Epoch 48/50
48636/48636 [==============================] - 23s 474us/sample - loss: 0.0827 - acc: 0.9735 - val_loss: 0.0651 - val_acc: 0.9803
Epoch 49/50
48636/48636 [==============================] - 23s 478us/sample - loss: 0.0818 - acc: 0.9739 - val_loss: 0.0686 - val_acc: 0.9805
Epoch 50/50
48636/48636 [==============================] - 24s 487us/sample - loss: 0.0776 - acc: 0.9759 - val_loss: 0.0676 - val_acc: 0.9798
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_20 (InputLayer)        (None, 20, 22, 1)         0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 10, 11, 32)        320       
_________________________________________________________________
batch_normalization_v1_3 (Ba (None, 10, 11, 32)        128       
_________________________________________________________________
dropout_18 (Dropout)         (None, 10, 11, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 5, 6, 32)          9248      
_________________________________________________________________
batch_normalization_v1_4 (Ba (None, 5, 6, 32)          128       
_________________________________________________________________
dropout_19 (Dropout)         (None, 5, 6, 32)          0         
_________________________________________________________________
flatten_18 (Flatten)         (None, 960)               0         
_________________________________________________________________
dropout_20 (Dropout)         (None, 960)               0         
_________________________________________________________________
dense_75 (Dense)             (None, 128)               123008    
_________________________________________________________________
batch_normalization_v1_5 (Ba (None, 128)               512       
_________________________________________________________________
dropout_21 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_76 (Dense)             (None, 8)                 1032      
=================================================================
Total params: 134,376
Trainable params: 133,992
Non-trainable params: 384
_________________________________________________________________
None
Confusion matrix, without normalization
[[1747   14    1    0    5    0    0   24]
 [  10 1712    3    0    0    0    4    1]
 [   1    1 1607    6    4    0    3    0]
 [   0    0    3  267    0    0    0    0]
 [   2    0    9    0  917    1    7    2]
 [   0    0    0    0    0 2012    2    0]
 [   0    0    3    0    1   13 1959    0]
 [ 114    4    2    0    4    2    0 1692]]