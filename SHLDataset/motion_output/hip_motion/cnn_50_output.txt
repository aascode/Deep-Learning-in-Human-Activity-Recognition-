Train on 48636 samples, validate on 12159 samples
Epoch 1/50
48636/48636 [==============================] - 10s 199us/sample - loss: 0.7132 - acc: 0.7417 - val_loss: 0.3698 - val_acc: 0.8659
Epoch 2/50
48636/48636 [==============================] - 10s 215us/sample - loss: 0.3831 - acc: 0.8498 - val_loss: 0.2677 - val_acc: 0.9047
Epoch 3/50
48636/48636 [==============================] - 11s 227us/sample - loss: 0.3196 - acc: 0.8773 - val_loss: 0.2340 - val_acc: 0.9138
Epoch 4/50
48636/48636 [==============================] - 11s 226us/sample - loss: 0.2833 - acc: 0.8915 - val_loss: 0.2096 - val_acc: 0.9317
Epoch 5/50
48636/48636 [==============================] - 11s 224us/sample - loss: 0.2609 - acc: 0.9017 - val_loss: 0.2075 - val_acc: 0.9251
Epoch 6/50
48636/48636 [==============================] - 11s 227us/sample - loss: 0.2454 - acc: 0.9065 - val_loss: 0.1855 - val_acc: 0.9331
Epoch 7/50
48636/48636 [==============================] - 11s 226us/sample - loss: 0.2329 - acc: 0.9121 - val_loss: 0.1859 - val_acc: 0.9345
Epoch 8/50
48636/48636 [==============================] - 11s 222us/sample - loss: 0.2231 - acc: 0.9144 - val_loss: 0.1789 - val_acc: 0.9372
Epoch 9/50
48636/48636 [==============================] - 11s 225us/sample - loss: 0.2163 - acc: 0.9186 - val_loss: 0.1674 - val_acc: 0.9396
Epoch 10/50
48636/48636 [==============================] - 11s 225us/sample - loss: 0.2122 - acc: 0.9181 - val_loss: 0.1679 - val_acc: 0.9382
Epoch 11/50
48636/48636 [==============================] - 11s 227us/sample - loss: 0.2036 - acc: 0.9220 - val_loss: 0.1599 - val_acc: 0.9419
Epoch 12/50
48636/48636 [==============================] - 11s 232us/sample - loss: 0.2003 - acc: 0.9231 - val_loss: 0.1583 - val_acc: 0.9439
Epoch 13/50
48636/48636 [==============================] - 11s 231us/sample - loss: 0.1948 - acc: 0.9260 - val_loss: 0.1537 - val_acc: 0.9440
Epoch 14/50
48636/48636 [==============================] - 11s 232us/sample - loss: 0.1918 - acc: 0.9263 - val_loss: 0.1586 - val_acc: 0.9396
Epoch 15/50
48636/48636 [==============================] - 12s 240us/sample - loss: 0.1857 - acc: 0.9286 - val_loss: 0.1518 - val_acc: 0.9473
Epoch 16/50
48636/48636 [==============================] - 12s 241us/sample - loss: 0.1855 - acc: 0.9294 - val_loss: 0.1448 - val_acc: 0.9482
Epoch 17/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.1827 - acc: 0.9305 - val_loss: 0.1460 - val_acc: 0.9470
Epoch 18/50
48636/48636 [==============================] - 12s 247us/sample - loss: 0.1802 - acc: 0.9317 - val_loss: 0.1464 - val_acc: 0.9478
Epoch 19/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.1766 - acc: 0.9324 - val_loss: 0.1412 - val_acc: 0.9497
Epoch 20/50
48636/48636 [==============================] - 12s 238us/sample - loss: 0.1734 - acc: 0.9341 - val_loss: 0.1417 - val_acc: 0.9507
Epoch 21/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.1708 - acc: 0.9352 - val_loss: 0.1408 - val_acc: 0.9519
Epoch 22/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.1688 - acc: 0.9358 - val_loss: 0.1336 - val_acc: 0.9536
Epoch 23/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.1648 - acc: 0.9374 - val_loss: 0.1327 - val_acc: 0.9528
Epoch 24/50
48636/48636 [==============================] - 11s 234us/sample - loss: 0.1636 - acc: 0.9371 - val_loss: 0.1364 - val_acc: 0.9510
Epoch 25/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.1613 - acc: 0.9379 - val_loss: 0.1346 - val_acc: 0.9524
Epoch 26/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.1600 - acc: 0.9391 - val_loss: 0.1291 - val_acc: 0.9566
Epoch 27/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.1593 - acc: 0.9392 - val_loss: 0.1289 - val_acc: 0.9563
Epoch 28/50
48636/48636 [==============================] - 12s 244us/sample - loss: 0.1572 - acc: 0.9395 - val_loss: 0.1330 - val_acc: 0.9526
Epoch 29/50
48636/48636 [==============================] - 12s 247us/sample - loss: 0.1575 - acc: 0.9384 - val_loss: 0.1287 - val_acc: 0.9551
Epoch 30/50
48636/48636 [==============================] - 12s 241us/sample - loss: 0.1510 - acc: 0.9415 - val_loss: 0.1256 - val_acc: 0.9576
Epoch 31/50
48636/48636 [==============================] - 12s 241us/sample - loss: 0.1545 - acc: 0.9412 - val_loss: 0.1273 - val_acc: 0.9546
Epoch 32/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.1479 - acc: 0.9420 - val_loss: 0.1211 - val_acc: 0.9594
Epoch 33/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.1485 - acc: 0.9433 - val_loss: 0.1241 - val_acc: 0.9560
Epoch 34/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.1459 - acc: 0.9435 - val_loss: 0.1223 - val_acc: 0.9588
Epoch 35/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.1464 - acc: 0.9435 - val_loss: 0.1211 - val_acc: 0.9575
Epoch 36/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.1439 - acc: 0.9450 - val_loss: 0.1277 - val_acc: 0.9557
Epoch 37/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.1480 - acc: 0.9435 - val_loss: 0.1261 - val_acc: 0.9555
Epoch 38/50
48636/48636 [==============================] - 12s 244us/sample - loss: 0.1448 - acc: 0.9447 - val_loss: 0.1211 - val_acc: 0.9587
Epoch 39/50
48636/48636 [==============================] - 12s 239us/sample - loss: 0.1421 - acc: 0.9457 - val_loss: 0.1209 - val_acc: 0.9563
Epoch 40/50
48636/48636 [==============================] - 12s 243us/sample - loss: 0.1407 - acc: 0.9460 - val_loss: 0.1175 - val_acc: 0.9609
Epoch 41/50
48636/48636 [==============================] - 12s 247us/sample - loss: 0.1420 - acc: 0.9446 - val_loss: 0.1160 - val_acc: 0.9618
Epoch 42/50
48636/48636 [==============================] - 12s 246us/sample - loss: 0.1393 - acc: 0.9468 - val_loss: 0.1173 - val_acc: 0.9605
Epoch 43/50
48636/48636 [==============================] - 12s 248us/sample - loss: 0.1397 - acc: 0.9459 - val_loss: 0.1201 - val_acc: 0.9608
Epoch 44/50
48636/48636 [==============================] - 12s 242us/sample - loss: 0.1349 - acc: 0.9473 - val_loss: 0.1192 - val_acc: 0.9598
Epoch 45/50
48636/48636 [==============================] - 12s 246us/sample - loss: 0.1369 - acc: 0.9472 - val_loss: 0.1165 - val_acc: 0.9617
Epoch 46/50
48636/48636 [==============================] - 12s 252us/sample - loss: 0.1347 - acc: 0.9472 - val_loss: 0.1180 - val_acc: 0.9605
Epoch 47/50
48636/48636 [==============================] - 11s 236us/sample - loss: 0.1367 - acc: 0.9485 - val_loss: 0.1140 - val_acc: 0.9610
Epoch 48/50
48636/48636 [==============================] - 12s 238us/sample - loss: 0.1304 - acc: 0.9499 - val_loss: 0.1133 - val_acc: 0.9621
Epoch 49/50
48636/48636 [==============================] - 12s 241us/sample - loss: 0.1321 - acc: 0.9485 - val_loss: 0.1110 - val_acc: 0.9627
Epoch 50/50
48636/48636 [==============================] - 12s 237us/sample - loss: 0.1342 - acc: 0.9486 - val_loss: 0.1181 - val_acc: 0.9603
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_29 (InputLayer)        (None, 20, 22, 1)         0         
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 10, 11, 16)        160       
_________________________________________________________________
batch_normalization_v1_21 (B (None, 10, 11, 16)        64        
_________________________________________________________________
dropout_42 (Dropout)         (None, 10, 11, 16)        0         
_________________________________________________________________
flatten_24 (Flatten)         (None, 1760)              0         
_________________________________________________________________
dropout_43 (Dropout)         (None, 1760)              0         
_________________________________________________________________
dense_91 (Dense)             (None, 128)               225408    
_________________________________________________________________
batch_normalization_v1_22 (B (None, 128)               512       
_________________________________________________________________
dropout_44 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_92 (Dense)             (None, 8)                 1032      
=================================================================
Total params: 227,176
Trainable params: 226,888
Non-trainable params: 288
_________________________________________________________________
None
Confusion matrix, without normalization
[[1634   14    5    0    2    0    0  136]
 [  33 1669    1    0    3    0   13   11]
 [   0    0 1574    9    8    0   31    0]
 [   0    0    1  269    0    0    0    0]
 [   3    0    4    0  912    0   14    5]
 [   0    0    0    0    0 2011    3    0]
 [   0   10    3    0   12    8 1943    0]
 [ 140    1    0    0   13    0    0 1664]]