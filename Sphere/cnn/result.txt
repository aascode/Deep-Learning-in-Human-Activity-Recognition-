11501/11501 [==============================] - 10s 856us/sample - loss: 0.7595 - acc: 0.8114 - val_loss: 0.7831 - val_acc: 0.7931
Epoch 45/200
11501/11501 [==============================] - 9s 826us/sample - loss: 0.7593 - acc: 0.8143 - val_loss: 0.7802 - val_acc: 0.8157
Epoch 46/200
11501/11501 [==============================] - 9s 822us/sample - loss: 0.7497 - acc: 0.8140 - val_loss: 0.7363 - val_acc: 0.8088
Epoch 47/200
11501/11501 [==============================] - 9s 823us/sample - loss: 0.7452 - acc: 0.8183 - val_loss: 0.7338 - val_acc: 0.8140
Epoch 48/200
11501/11501 [==============================] - 9s 822us/sample - loss: 0.7376 - acc: 0.8186 - val_loss: 0.7942 - val_acc: 0.7917
Epoch 49/200
11501/11501 [==============================] - 9s 820us/sample - loss: 0.7451 - acc: 0.8122 - val_loss: 0.7518 - val_acc: 0.8088
Epoch 50/200
11501/11501 [==============================] - 9s 819us/sample - loss: 0.7386 - acc: 0.8170 - val_loss: 0.7480 - val_acc: 0.8053
Epoch 51/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.7366 - acc: 0.8139 - val_loss: 0.6981 - val_acc: 0.8241
Epoch 52/200
11501/11501 [==============================] - 9s 820us/sample - loss: 0.7199 - acc: 0.8179 - val_loss: 0.7145 - val_acc: 0.8199
Epoch 53/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.7321 - acc: 0.8174 - val_loss: 0.7484 - val_acc: 0.8202
Epoch 54/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.7335 - acc: 0.8162 - val_loss: 0.6922 - val_acc: 0.8175
Epoch 55/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.7159 - acc: 0.8185 - val_loss: 0.7462 - val_acc: 0.8108
Epoch 56/200
11501/11501 [==============================] - 10s 844us/sample - loss: 0.7328 - acc: 0.8160 - val_loss: 0.7755 - val_acc: 0.7921
Epoch 57/200
11501/11501 [==============================] - 10s 848us/sample - loss: 0.7287 - acc: 0.8201 - val_loss: 0.7961 - val_acc: 0.7876
Epoch 58/200
11501/11501 [==============================] - 10s 861us/sample - loss: 0.7277 - acc: 0.8196 - val_loss: 0.7137 - val_acc: 0.8202
Epoch 59/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.7238 - acc: 0.8197 - val_loss: 0.9158 - val_acc: 0.7448
Epoch 60/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.7215 - acc: 0.8180 - val_loss: 0.7014 - val_acc: 0.8241 ETA: 3s - loss: 0.7108 - acc: 0.8195
Epoch 61/200
11501/11501 [==============================] - 10s 826us/sample - loss: 0.7294 - acc: 0.8173 - val_loss: 0.9061 - val_acc: 0.7611
Epoch 62/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.7351 - acc: 0.8181 - val_loss: 0.7075 - val_acc: 0.8129
Epoch 63/200
11501/11501 [==============================] - 10s 828us/sample - loss: 0.7168 - acc: 0.8204 - val_loss: 0.7630 - val_acc: 0.7994
Epoch 64/200
11501/11501 [==============================] - 10s 828us/sample - loss: 0.7166 - acc: 0.8208 - val_loss: 0.7579 - val_acc: 0.8032
Epoch 65/200
11501/11501 [==============================] - 10s 851us/sample - loss: 0.7183 - acc: 0.8176 - val_loss: 0.6856 - val_acc: 0.8268
Epoch 66/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.7230 - acc: 0.8184 - val_loss: 0.7676 - val_acc: 0.7928
Epoch 67/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.7175 - acc: 0.8212 - val_loss: 0.7027 - val_acc: 0.8199
Epoch 68/200
11501/11501 [==============================] - 10s 841us/sample - loss: 0.7045 - acc: 0.8225 - val_loss: 0.7256 - val_acc: 0.8136
Epoch 69/200
11501/11501 [==============================] - 10s 853us/sample - loss: 0.7240 - acc: 0.8163 - val_loss: 0.7030 - val_acc: 0.8185
Epoch 70/200
11501/11501 [==============================] - 10s 845us/sample - loss: 0.7213 - acc: 0.8195 - val_loss: 0.7015 - val_acc: 0.8220
Epoch 71/200
11501/11501 [==============================] - 10s 851us/sample - loss: 0.7190 - acc: 0.8151 - val_loss: 0.7217 - val_acc: 0.8157
Epoch 72/200
11501/11501 [==============================] - 10s 851us/sample - loss: 0.7118 - acc: 0.8199 - val_loss: 0.7229 - val_acc: 0.8115
Epoch 73/200
11501/11501 [==============================] - 10s 839us/sample - loss: 0.7120 - acc: 0.8201 - val_loss: 0.6804 - val_acc: 0.8199
Epoch 74/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.7033 - acc: 0.8180 - val_loss: 0.6850 - val_acc: 0.8265
Epoch 75/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.7039 - acc: 0.8226 - val_loss: 0.6608 - val_acc: 0.8261
Epoch 76/200
11501/11501 [==============================] - 10s 828us/sample - loss: 0.7045 - acc: 0.8203 - val_loss: 0.6750 - val_acc: 0.8261
Epoch 77/200
11501/11501 [==============================] - 10s 846us/sample - loss: 0.7168 - acc: 0.8168 - val_loss: 0.7410 - val_acc: 0.8022
Epoch 78/200
11501/11501 [==============================] - 10s 862us/sample - loss: 0.7068 - acc: 0.8226 - val_loss: 0.7004 - val_acc: 0.8122
Epoch 79/200
11501/11501 [==============================] - 10s 849us/sample - loss: 0.6998 - acc: 0.8232 - val_loss: 0.7281 - val_acc: 0.8108
Epoch 80/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6940 - acc: 0.8238 - val_loss: 0.6775 - val_acc: 0.8237
Epoch 81/200
11501/11501 [==============================] - 10s 845us/sample - loss: 0.7054 - acc: 0.8222 - val_loss: 0.7107 - val_acc: 0.8171
Epoch 82/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.7016 - acc: 0.8223 - val_loss: 0.6906 - val_acc: 0.8293
Epoch 83/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.7015 - acc: 0.8236 - val_loss: 0.6891 - val_acc: 0.8195
Epoch 84/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6944 - acc: 0.8232 - val_loss: 0.6776 - val_acc: 0.8334
Epoch 85/200
11501/11501 [==============================] - 10s 851us/sample - loss: 0.7005 - acc: 0.8231 - val_loss: 0.6958 - val_acc: 0.8188
Epoch 86/200
11501/11501 [==============================] - 9s 821us/sample - loss: 0.7035 - acc: 0.8197 - val_loss: 0.6760 - val_acc: 0.8268
Epoch 87/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.7070 - acc: 0.8252 - val_loss: 0.7129 - val_acc: 0.8157
Epoch 88/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.7045 - acc: 0.8204 - val_loss: 0.6591 - val_acc: 0.8331
Epoch 89/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.7092 - acc: 0.8205 - val_loss: 0.7115 - val_acc: 0.8230
Epoch 90/200
11501/11501 [==============================] - 9s 826us/sample - loss: 0.7066 - acc: 0.8198 - val_loss: 0.8648 - val_acc: 0.7729
Epoch 91/200
11501/11501 [==============================] - 10s 842us/sample - loss: 0.7032 - acc: 0.8232 - val_loss: 0.6685 - val_acc: 0.8195
Epoch 92/200
11501/11501 [==============================] - 10s 868us/sample - loss: 0.7249 - acc: 0.8197 - val_loss: 0.6976 - val_acc: 0.8216
Epoch 93/200
11501/11501 [==============================] - 10s 841us/sample - loss: 0.7019 - acc: 0.8254 - val_loss: 0.6644 - val_acc: 0.8227
Epoch 94/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6916 - acc: 0.8230 - val_loss: 0.6978 - val_acc: 0.8241
Epoch 95/200
11501/11501 [==============================] - 9s 824us/sample - loss: 0.6923 - acc: 0.8248 - val_loss: 0.7077 - val_acc: 0.8164
Epoch 96/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.6945 - acc: 0.8226 - val_loss: 0.6807 - val_acc: 0.8286
Epoch 97/200
11501/11501 [==============================] - 10s 826us/sample - loss: 0.6926 - acc: 0.8246 - val_loss: 0.6808 - val_acc: 0.8220
Epoch 98/200
11501/11501 [==============================] - 9s 819us/sample - loss: 0.6815 - acc: 0.8238 - val_loss: 0.7180 - val_acc: 0.8115
Epoch 99/200
11501/11501 [==============================] - 10s 860us/sample - loss: 0.6936 - acc: 0.8210 - val_loss: 0.7576 - val_acc: 0.8115
Epoch 100/200
11501/11501 [==============================] - 10s 843us/sample - loss: 0.6957 - acc: 0.8257 - val_loss: 0.7621 - val_acc: 0.8102
Epoch 101/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6960 - acc: 0.8245 - val_loss: 0.8873 - val_acc: 0.7479
Epoch 102/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.6951 - acc: 0.8223 - val_loss: 0.7707 - val_acc: 0.8011
Epoch 103/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.6858 - acc: 0.8254 - val_loss: 0.6527 - val_acc: 0.8331
Epoch 104/200
11501/11501 [==============================] - 9s 822us/sample - loss: 0.6913 - acc: 0.8245 - val_loss: 0.7259 - val_acc: 0.8115
Epoch 105/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.6892 - acc: 0.8238 - val_loss: 0.6877 - val_acc: 0.8168
Epoch 106/200
11501/11501 [==============================] - 10s 842us/sample - loss: 0.6966 - acc: 0.8231 - val_loss: 0.7081 - val_acc: 0.8150
Epoch 107/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6844 - acc: 0.8269 - val_loss: 0.7145 - val_acc: 0.8049
Epoch 108/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.6843 - acc: 0.8231 - val_loss: 0.6763 - val_acc: 0.8216
Epoch 109/200
11501/11501 [==============================] - 10s 826us/sample - loss: 0.6854 - acc: 0.8266 - val_loss: 0.6574 - val_acc: 0.8324
Epoch 110/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6870 - acc: 0.8253 - val_loss: 0.7331 - val_acc: 0.8119
Epoch 111/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6908 - acc: 0.8232 - val_loss: 0.6955 - val_acc: 0.8091
Epoch 112/200
11501/11501 [==============================] - 10s 843us/sample - loss: 0.7006 - acc: 0.8217 - val_loss: 0.6712 - val_acc: 0.8307
Epoch 113/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6984 - acc: 0.8233 - val_loss: 0.6740 - val_acc: 0.8255
Epoch 114/200
11501/11501 [==============================] - 9s 824us/sample - loss: 0.6977 - acc: 0.8213 - val_loss: 0.6751 - val_acc: 0.8213
Epoch 115/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.6888 - acc: 0.8251 - val_loss: 0.7032 - val_acc: 0.8185
Epoch 116/200
11501/11501 [==============================] - 10s 828us/sample - loss: 0.6979 - acc: 0.8190 - val_loss: 0.6825 - val_acc: 0.8209
Epoch 117/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6961 - acc: 0.8223 - val_loss: 0.7313 - val_acc: 0.8074
Epoch 118/200
11501/11501 [==============================] - 9s 824us/sample - loss: 0.7001 - acc: 0.8200 - val_loss: 0.6648 - val_acc: 0.8244
Epoch 119/200
11501/11501 [==============================] - 10s 853us/sample - loss: 0.6849 - acc: 0.8263 - val_loss: 0.6805 - val_acc: 0.8251
Epoch 120/200
11501/11501 [==============================] - 9s 826us/sample - loss: 0.6880 - acc: 0.8252 - val_loss: 0.6935 - val_acc: 0.8202
Epoch 121/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6831 - acc: 0.8258 - val_loss: 0.6427 - val_acc: 0.8303
Epoch 122/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.6963 - acc: 0.8247 - val_loss: 0.7297 - val_acc: 0.8143
Epoch 123/200
11501/11501 [==============================] - 9s 823us/sample - loss: 0.6793 - acc: 0.8278 - val_loss: 0.6591 - val_acc: 0.8307
Epoch 124/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.6828 - acc: 0.8252 - val_loss: 0.6820 - val_acc: 0.8241
Epoch 125/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.6842 - acc: 0.8264 - val_loss: 0.6680 - val_acc: 0.8175
Epoch 126/200
11501/11501 [==============================] - 10s 853us/sample - loss: 0.6674 - acc: 0.8308 - val_loss: 0.6580 - val_acc: 0.8286
Epoch 127/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.6802 - acc: 0.8238 - val_loss: 0.6674 - val_acc: 0.8286
Epoch 128/200
11501/11501 [==============================] - 10s 827us/sample - loss: 0.6743 - acc: 0.8307 - val_loss: 0.7051 - val_acc: 0.8129
Epoch 129/200
11501/11501 [==============================] - 9s 824us/sample - loss: 0.6873 - acc: 0.8250 - val_loss: 0.6738 - val_acc: 0.8279
Epoch 130/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.6765 - acc: 0.8282 - val_loss: 0.6622 - val_acc: 0.8268
Epoch 131/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.6802 - acc: 0.8235 - val_loss: 0.6781 - val_acc: 0.8282
Epoch 132/200
11501/11501 [==============================] - 10s 854us/sample - loss: 0.6784 - acc: 0.8237 - val_loss: 0.6412 - val_acc: 0.8300
Epoch 133/200
11501/11501 [==============================] - 10s 874us/sample - loss: 0.6838 - acc: 0.8232 - val_loss: 0.6584 - val_acc: 0.8275
Epoch 134/200
11501/11501 [==============================] - 10s 860us/sample - loss: 0.6806 - acc: 0.8245 - val_loss: 0.6851 - val_acc: 0.8234
Epoch 135/200
11501/11501 [==============================] - 10s 847us/sample - loss: 0.6855 - acc: 0.8242 - val_loss: 0.7543 - val_acc: 0.8161
Epoch 136/200
11501/11501 [==============================] - 10s 841us/sample - loss: 0.6687 - acc: 0.8290 - val_loss: 0.6724 - val_acc: 0.8286
Epoch 137/200
11501/11501 [==============================] - 10s 842us/sample - loss: 0.6865 - acc: 0.8248 - val_loss: 0.6813 - val_acc: 0.8258
Epoch 138/200
11501/11501 [==============================] - 10s 839us/sample - loss: 0.6688 - acc: 0.8302 - val_loss: 0.6582 - val_acc: 0.8300
Epoch 139/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6700 - acc: 0.8284 - val_loss: 0.6866 - val_acc: 0.8185
Epoch 140/200
11501/11501 [==============================] - 10s 864us/sample - loss: 0.6752 - acc: 0.8277 - val_loss: 0.6526 - val_acc: 0.8355
Epoch 141/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6712 - acc: 0.8287 - val_loss: 0.6691 - val_acc: 0.8241
Epoch 142/200
11501/11501 [==============================] - 10s 827us/sample - loss: 0.6736 - acc: 0.8287 - val_loss: 0.6568 - val_acc: 0.8237
Epoch 143/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6605 - acc: 0.8331 - val_loss: 0.6370 - val_acc: 0.8286
Epoch 144/200
11501/11501 [==============================] - 10s 828us/sample - loss: 0.6737 - acc: 0.8258 - val_loss: 0.6799 - val_acc: 0.8213
Epoch 145/200
11501/11501 [==============================] - 9s 825us/sample - loss: 0.6784 - acc: 0.8277 - val_loss: 0.6535 - val_acc: 0.8317
Epoch 146/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6579 - acc: 0.8306 - val_loss: 0.6957 - val_acc: 0.8261
Epoch 147/200
11501/11501 [==============================] - 10s 838us/sample - loss: 0.6613 - acc: 0.8290 - val_loss: 0.6504 - val_acc: 0.8282
Epoch 148/200
11501/11501 [==============================] - 9s 817us/sample - loss: 0.6800 - acc: 0.8264 - val_loss: 0.6572 - val_acc: 0.8275
Epoch 149/200
11501/11501 [==============================] - 10s 827us/sample - loss: 0.6700 - acc: 0.8255 - val_loss: 0.6654 - val_acc: 0.8206
Epoch 150/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6729 - acc: 0.8290 - val_loss: 0.6542 - val_acc: 0.8272
Epoch 151/200
11501/11501 [==============================] - 10s 844us/sample - loss: 0.6789 - acc: 0.8286 - val_loss: 0.6933 - val_acc: 0.8275
Epoch 152/200
11501/11501 [==============================] - 10s 855us/sample - loss: 0.6855 - acc: 0.8243 - val_loss: 0.6788 - val_acc: 0.8279
Epoch 153/200
11501/11501 [==============================] - 10s 866us/sample - loss: 0.6580 - acc: 0.8341 - val_loss: 0.6717 - val_acc: 0.8258
Epoch 154/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6848 - acc: 0.8271 - val_loss: 0.7968 - val_acc: 0.8084
Epoch 155/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.6644 - acc: 0.8327 - val_loss: 0.6449 - val_acc: 0.8352
Epoch 156/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.6755 - acc: 0.8306 - val_loss: 0.6510 - val_acc: 0.8303
Epoch 157/200
11501/11501 [==============================] - 9s 823us/sample - loss: 0.6675 - acc: 0.8293 - val_loss: 0.7240 - val_acc: 0.8129
Epoch 158/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.6686 - acc: 0.8301 - val_loss: 0.7068 - val_acc: 0.8202
Epoch 159/200
11501/11501 [==============================] - 9s 824us/sample - loss: 0.6796 - acc: 0.8247 - val_loss: 0.6464 - val_acc: 0.8355
Epoch 160/200
11501/11501 [==============================] - 10s 861us/sample - loss: 0.6612 - acc: 0.8324 - val_loss: 0.6795 - val_acc: 0.8202
Epoch 161/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.6628 - acc: 0.8299 - val_loss: 0.6562 - val_acc: 0.8279
Epoch 162/200
11501/11501 [==============================] - 10s 830us/sample - loss: 0.6667 - acc: 0.8269 - val_loss: 0.6596 - val_acc: 0.8272
Epoch 163/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.6671 - acc: 0.8274 - val_loss: 0.6901 - val_acc: 0.8314
Epoch 164/200
11501/11501 [==============================] - 10s 845us/sample - loss: 0.6741 - acc: 0.8287 - val_loss: 0.6397 - val_acc: 0.8355
Epoch 165/200
11501/11501 [==============================] - 10s 849us/sample - loss: 0.6581 - acc: 0.8329 - val_loss: 0.6700 - val_acc: 0.8341
Epoch 166/200
11501/11501 [==============================] - 10s 850us/sample - loss: 0.6752 - acc: 0.8304 - val_loss: 0.6531 - val_acc: 0.8345
Epoch 167/200
11501/11501 [==============================] - 10s 862us/sample - loss: 0.6751 - acc: 0.8299 - val_loss: 0.6387 - val_acc: 0.8289
Epoch 168/200
11501/11501 [==============================] - 10s 836us/sample - loss: 0.6670 - acc: 0.8322 - val_loss: 0.7118 - val_acc: 0.8237
Epoch 169/200
11501/11501 [==============================] - 10s 838us/sample - loss: 0.6483 - acc: 0.8325 - val_loss: 0.6216 - val_acc: 0.8303
Epoch 170/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6801 - acc: 0.8264 - val_loss: 0.6515 - val_acc: 0.8348
Epoch 171/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.6745 - acc: 0.8284 - val_loss: 0.6730 - val_acc: 0.8265
Epoch 172/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.6668 - acc: 0.8340 - val_loss: 0.6864 - val_acc: 0.8206
Epoch 173/200
11501/11501 [==============================] - 10s 831us/sample - loss: 0.6764 - acc: 0.8292 - val_loss: 0.8743 - val_acc: 0.7712
Epoch 174/200
11501/11501 [==============================] - 10s 858us/sample - loss: 0.6644 - acc: 0.8297 - val_loss: 0.6592 - val_acc: 0.8345
Epoch 175/200
11501/11501 [==============================] - 10s 846us/sample - loss: 0.6667 - acc: 0.8272 - val_loss: 0.7839 - val_acc: 0.7938
Epoch 176/200
11501/11501 [==============================] - 10s 851us/sample - loss: 0.6616 - acc: 0.8267 - val_loss: 0.6585 - val_acc: 0.8348
Epoch 177/200
11501/11501 [==============================] - 10s 844us/sample - loss: 0.6607 - acc: 0.8291 - val_loss: 0.6610 - val_acc: 0.8279
Epoch 178/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6765 - acc: 0.8275 - val_loss: 0.7274 - val_acc: 0.8182
Epoch 179/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.6470 - acc: 0.8334 - val_loss: 0.6323 - val_acc: 0.8328
Epoch 180/200
11501/11501 [==============================] - 10s 850us/sample - loss: 0.6600 - acc: 0.8296 - val_loss: 0.6778 - val_acc: 0.8314
Epoch 181/200
11501/11501 [==============================] - 10s 846us/sample - loss: 0.6640 - acc: 0.8310 - val_loss: 0.6776 - val_acc: 0.8213
Epoch 182/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.6683 - acc: 0.8274 - val_loss: 0.6686 - val_acc: 0.8275
Epoch 183/200
11501/11501 [==============================] - 10s 838us/sample - loss: 0.6636 - acc: 0.8325 - val_loss: 0.6412 - val_acc: 0.8366
Epoch 184/200
11501/11501 [==============================] - 10s 837us/sample - loss: 0.6644 - acc: 0.8311 - val_loss: 0.6838 - val_acc: 0.8261
Epoch 185/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6635 - acc: 0.8293 - val_loss: 0.6502 - val_acc: 0.8328
Epoch 186/200
11501/11501 [==============================] - 10s 839us/sample - loss: 0.6551 - acc: 0.8321 - val_loss: 0.7411 - val_acc: 0.8171
Epoch 187/200
11501/11501 [==============================] - 10s 871us/sample - loss: 0.6662 - acc: 0.8314 - val_loss: 0.7260 - val_acc: 0.8032
Epoch 188/200
11501/11501 [==============================] - 10s 854us/sample - loss: 0.6625 - acc: 0.8277 - val_loss: 0.6892 - val_acc: 0.8216
Epoch 189/200
11501/11501 [==============================] - 10s 850us/sample - loss: 0.6675 - acc: 0.8284 - val_loss: 0.7509 - val_acc: 0.8178
Epoch 190/200
11501/11501 [==============================] - 10s 843us/sample - loss: 0.6646 - acc: 0.8318 - val_loss: 0.6800 - val_acc: 0.8223
Epoch 191/200
11501/11501 [==============================] - 10s 833us/sample - loss: 0.6574 - acc: 0.8339 - val_loss: 0.6791 - val_acc: 0.8268
Epoch 192/200
11501/11501 [==============================] - 10s 834us/sample - loss: 0.6563 - acc: 0.8344 - val_loss: 0.6764 - val_acc: 0.8282
Epoch 193/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6613 - acc: 0.8339 - val_loss: 0.6759 - val_acc: 0.8248
Epoch 194/200
11501/11501 [==============================] - 10s 874us/sample - loss: 0.6518 - acc: 0.8326 - val_loss: 0.6450 - val_acc: 0.8279
Epoch 195/200
11501/11501 [==============================] - 10s 858us/sample - loss: 0.6668 - acc: 0.8316 - val_loss: 0.6615 - val_acc: 0.8286
Epoch 196/200
11501/11501 [==============================] - 10s 848us/sample - loss: 0.6577 - acc: 0.8295 - val_loss: 0.6575 - val_acc: 0.8307
Epoch 197/200
11501/11501 [==============================] - 10s 843us/sample - loss: 0.6673 - acc: 0.8334 - val_loss: 0.6561 - val_acc: 0.8230
Epoch 198/200
11501/11501 [==============================] - 10s 840us/sample - loss: 0.6507 - acc: 0.8348 - val_loss: 0.7018 - val_acc: 0.8293
Epoch 199/200
11501/11501 [==============================] - 10s 832us/sample - loss: 0.6636 - acc: 0.8293 - val_loss: 0.6476 - val_acc: 0.8359
Epoch 200/200
11501/11501 [==============================] - 10s 835us/sample - loss: 0.6608 - acc: 0.8302 - val_loss: 0.6399 - val_acc: 0.8310
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        (None, 20, 23, 1)         0         
_________________________________________________________________
conv2d_56 (Conv2D)           (None, 10, 12, 32)        320       
_________________________________________________________________
batch_normalization_v1_56 (B (None, 10, 12, 32)        128       
_________________________________________________________________
max_pooling2d_15 (MaxPooling (None, 5, 6, 32)          0         
_________________________________________________________________
dropout_30 (Dropout)         (None, 5, 6, 32)          0         
_________________________________________________________________
conv2d_57 (Conv2D)           (None, 3, 3, 64)          18496     
_________________________________________________________________
batch_normalization_v1_57 (B (None, 3, 3, 64)          256       
_________________________________________________________________
dropout_31 (Dropout)         (None, 3, 3, 64)          0         
_________________________________________________________________
conv2d_58 (Conv2D)           (None, 2, 2, 256)         147712    
_________________________________________________________________
batch_normalization_v1_58 (B (None, 2, 2, 256)         1024      
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 1, 1, 256)         0         
_________________________________________________________________
conv2d_59 (Conv2D)           (None, 1, 1, 128)         295040    
_________________________________________________________________
batch_normalization_v1_59 (B (None, 1, 1, 128)         512       
_________________________________________________________________
flatten_14 (Flatten)         (None, 128)               0         
_________________________________________________________________
dropout_32 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_34 (Dense)             (None, 1024)              132096    
_________________________________________________________________
dropout_33 (Dropout)         (None, 1024)              0         
_________________________________________________________________
dense_35 (Dense)             (None, 20)                20500     
=================================================================
Total params: 616,084
Trainable params: 615,124
Non-trainable params: 960
_________________________________________________________________
None
Confusion matrix, without normalization
[[   8    0    0    1    1    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0]
 [   1   15    0    1    2    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0]
 [   0    0    2    0    0    0    0    0    0    0    6    0    0    0
     0    0    0    0    0    0]
 [   0    0    0    4   15    0    1    0    0    0    7    0    0    0
     0    0    0    0    0    0]
 [   1    1    0    7  141    3    0    0    1    0  106    0    0    0
     0    0    0    0    0    0]
 [   0    0    0    0    3   71    1    0    0    0   12    0    0    0
     0    0    0    0    0    0]
 [   0    0    0    0    0    1   24    0    3    0    5    0    0    0
     0    1    0    0    0    0]
 [   0    0    0    0    0    0    0  211    5    0    0    0    0    1
     0    0    0    0    0    1]
 [   0    0    0    0    1    2    3    3  744    0    5    1    0    0
     0    0    0    1    0    0]
 [   0    0    0    0    0    0    2    0    2    6    0    0    0    0
     0    0    0    0    0    0]
 [   0    0    1    0   25    4    0    0    0    0 1139    0    0    0
     0    0    0    0    0    0]
 [   0    0    0    0    1    7    0    0    1    0    7    0    0    0
     0    0    0    1    0    0]
 [   0    0    0    0    0    0    1    0    0    0   13    0    0    0
     0    0    0    0    0    0]
 [   0    0    0    0    0    0    0    6    8    0    0    0    0    7
     0    0    0    0    0    0]
 [   0    0    0    0    0    0    0    8    6    0    0    0    0    4
     7    0    0    0    0    1]
 [   0    0    0    0    0    0    0    0    4    0   10    0    0    0
     0    2    0    0    0    0]
 [   0    0    0    0    0    0   10    0    0    0    5    0    0    0
     0    0    1    0    0    0]
 [   0    0    0    0    2    0    0    2    6    0    5    0    0    0
     0    0    0    6    0    0]
 [   0    0    0    0    5    3    0    0    1    0   12    0    2    0
     0    1    0    0    0    0]
 [   0    0    0    1   14    0    0   14   12    0   76    0    0    0
     0    0    0    0    0    2]]