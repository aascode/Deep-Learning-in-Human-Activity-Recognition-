read h5 file....
(32178, 25, 40)
Train on 32178 samples, validate on 21452 samples
Epoch 1/50
32178/32178 [==============================] - 36s 1ms/sample - loss: 0.6296 - acc: 0.8229 - val_loss: 0.4189 - val_acc: 0.8869
Epoch 2/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.3951 - acc: 0.8976 - val_loss: 0.4141 - val_acc: 0.8879
Epoch 3/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.3465 - acc: 0.9160 - val_loss: 0.3110 - val_acc: 0.9264
Epoch 4/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.3139 - acc: 0.9277 - val_loss: 0.2615 - val_acc: 0.9447
Epoch 5/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.2992 - acc: 0.9331 - val_loss: 0.2700 - val_acc: 0.9440
Epoch 6/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.2805 - acc: 0.9422 - val_loss: 0.2584 - val_acc: 0.9496
Epoch 7/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.2722 - acc: 0.9448 - val_loss: 0.2521 - val_acc: 0.9522
Epoch 8/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2719 - acc: 0.9473 - val_loss: 0.2491 - val_acc: 0.9515
Epoch 9/50
32178/32178 [==============================] - 36s 1ms/sample - loss: 0.2613 - acc: 0.9487 - val_loss: 0.2661 - val_acc: 0.9495
Epoch 10/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2523 - acc: 0.9514 - val_loss: 0.2584 - val_acc: 0.9475
Epoch 11/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2450 - acc: 0.9558 - val_loss: 0.2341 - val_acc: 0.9595
Epoch 12/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2504 - acc: 0.9526 - val_loss: 0.2203 - val_acc: 0.9632
Epoch 13/50
32178/32178 [==============================] - 36s 1ms/sample - loss: 0.2370 - acc: 0.9571 - val_loss: 0.2147 - val_acc: 0.9656
Epoch 14/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2326 - acc: 0.9581 - val_loss: 0.2731 - val_acc: 0.9490
Epoch 15/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.2335 - acc: 0.9575 - val_loss: 0.2582 - val_acc: 0.9548
Epoch 16/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2260 - acc: 0.9599 - val_loss: 0.2040 - val_acc: 0.9674
Epoch 17/50
32178/32178 [==============================] - 37s 1ms/sample - loss: 0.2250 - acc: 0.9594 - val_loss: 0.2388 - val_acc: 0.9587
Epoch 18/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.2282 - acc: 0.9582 - val_loss: 0.2111 - val_acc: 0.9656
Epoch 19/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.2171 - acc: 0.9625 - val_loss: 0.2026 - val_acc: 0.9669
Epoch 20/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.2111 - acc: 0.9644 - val_loss: 0.2104 - val_acc: 0.9635
Epoch 21/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.2080 - acc: 0.9639 - val_loss: 0.2132 - val_acc: 0.9659
Epoch 22/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.2189 - acc: 0.9621 - val_loss: 0.1940 - val_acc: 0.9710
Epoch 23/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.2052 - acc: 0.9662 - val_loss: 0.1844 - val_acc: 0.9736
Epoch 24/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.2062 - acc: 0.9646 - val_loss: 0.1937 - val_acc: 0.9697
Epoch 25/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1989 - acc: 0.9660 - val_loss: 0.2116 - val_acc: 0.9651
Epoch 26/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.2051 - acc: 0.9646 - val_loss: 0.1848 - val_acc: 0.9733
Epoch 27/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1964 - acc: 0.9665 - val_loss: 0.2122 - val_acc: 0.9658
Epoch 28/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1902 - acc: 0.9689 - val_loss: 0.2354 - val_acc: 0.9547
Epoch 29/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.1875 - acc: 0.9694 - val_loss: 0.1958 - val_acc: 0.9695
Epoch 30/50
32178/32178 [==============================] - 41s 1ms/sample - loss: 0.1911 - acc: 0.9670 - val_loss: 0.2875 - val_acc: 0.9337
Epoch 31/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.1954 - acc: 0.9671 - val_loss: 0.1936 - val_acc: 0.9678
Epoch 32/50
32178/32178 [==============================] - 43s 1ms/sample - loss: 0.1896 - acc: 0.9685 - val_loss: 0.1977 - val_acc: 0.9692
Epoch 33/50
32178/32178 [==============================] - 44s 1ms/sample - loss: 0.1803 - acc: 0.9706 - val_loss: 0.1980 - val_acc: 0.9683
Epoch 34/50
32178/32178 [==============================] - 43s 1ms/sample - loss: 0.1880 - acc: 0.9689 - val_loss: 0.1863 - val_acc: 0.9708
Epoch 35/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1817 - acc: 0.9700 - val_loss: 0.1758 - val_acc: 0.9735
Epoch 36/50
32178/32178 [==============================] - 41s 1ms/sample - loss: 0.1772 - acc: 0.9710 - val_loss: 0.1804 - val_acc: 0.9738
Epoch 37/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1750 - acc: 0.9711 - val_loss: 0.1651 - val_acc: 0.9738
Epoch 38/50
32178/32178 [==============================] - 41s 1ms/sample - loss: 0.1820 - acc: 0.9692 - val_loss: 0.1861 - val_acc: 0.9704
Epoch 39/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.1744 - acc: 0.9709 - val_loss: 0.1684 - val_acc: 0.9759
Epoch 40/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.1785 - acc: 0.9701 - val_loss: 0.1837 - val_acc: 0.9717
Epoch 41/50
32178/32178 [==============================] - 41s 1ms/sample - loss: 0.1811 - acc: 0.9704 - val_loss: 0.1887 - val_acc: 0.9682
Epoch 42/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1686 - acc: 0.9734 - val_loss: 0.1776 - val_acc: 0.9711
Epoch 43/50
32178/32178 [==============================] - 42s 1ms/sample - loss: 0.1734 - acc: 0.9721 - val_loss: 0.1663 - val_acc: 0.9757
Epoch 44/50
32178/32178 [==============================] - 40s 1ms/sample - loss: 0.1712 - acc: 0.9723 - val_loss: 0.1786 - val_acc: 0.9703
Epoch 45/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1669 - acc: 0.9729 - val_loss: 0.1611 - val_acc: 0.9766
Epoch 46/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1634 - acc: 0.9741 - val_loss: 0.1639 - val_acc: 0.9767
Epoch 47/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1603 - acc: 0.9745 - val_loss: 0.1605 - val_acc: 0.9773
Epoch 48/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1630 - acc: 0.9736 - val_loss: 0.1607 - val_acc: 0.9759
Epoch 49/50
32178/32178 [==============================] - 38s 1ms/sample - loss: 0.1654 - acc: 0.9737 - val_loss: 0.1646 - val_acc: 0.9750
Epoch 50/50
32178/32178 [==============================] - 39s 1ms/sample - loss: 0.1647 - acc: 0.9725 - val_loss: 0.1743 - val_acc: 0.9747
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         (None, 25, 40, 1)         0         
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 13, 20, 32)        320       
_________________________________________________________________
batch_normalization_v1_12 (B (None, 13, 20, 32)        128       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 6, 10, 32)         0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 3, 5, 64)          18496     
_________________________________________________________________
batch_normalization_v1_13 (B (None, 3, 5, 64)          256       
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 2, 3, 128)         73856     
_________________________________________________________________
batch_normalization_v1_14 (B (None, 2, 3, 128)         512       
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 1, 1, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 128)               0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 1024)              132096    
_________________________________________________________________
dropout_10 (Dropout)         (None, 1024)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 12)                12300     
=================================================================
Total params: 237,964
Trainable params: 237,516
Non-trainable params: 448